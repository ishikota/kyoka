{
    "docs": [
        {
            "location": "/", 
            "text": "kyoka - Reinforcement Learning framework\n\n\nWhat is Reinforcement Learning\n\n\n\n\nReinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. (wikipedia)\n\n\n\n\nIn reinforcement learning, the player to learn how to get good result in some task is called as \nagent\n.\n\nAgent learns which action is good or bad at each situation through large number of simulation.\n\n(The essential factor to characterize reinforcement learning is \nlearning from trial-and-error\n.)\n\n\nWhy \nkyoka\n is creted\n\n\nThe steps to solve your learning problem (ex. playing Go) by reinforcement learning algorithms would be  \n\n\n\n\nDefine your learning problem\n in Reinforcement Learning format.  \n\n\nSelect learning algorithm(ex. QLearning) and \nimplement it for your learning problem\n.  \n\n\n\n\nWe have a lots of things to do before start learning.\n\nThis library is created to ease implementing these steps.  \n\n\nSorry, I talked too much. Let's see the code with simple example !!\n\n\nHello Reinforcement Learning\n\n\nWe will find the shortest path to escape from below maze by \nQLearning\n.\n\n\nS: start, G: goal, X: wall\n\n-------XG\n--X----X-\nS-X----X-\n--X------\n-----X---\n---------\n\n\n\n\nStep1. Define Maze Task\n\n\nFirst we define our learning problem as reinforcement learning task.\n\nkyoka\n provides \nkyoka.task.BaseTask\n template class. This class has 5 abstracted methods you need to implement.\n\n\n\n\ngegenerate_inital_state\n : define start state of our problem\n\n\nis_terminal_state\n : define when is the finish of our problem\n\n\ntransit_state\n : define the rule of state transition in our problem\n\n\ngenerate_possible_actions\n : define what action is possible in each state\n\n\ncalculate_reward\n : define how good each state is\n\n\n\n\nHere is the \nMazeTask\n class which represents our learning problem.\n\n\nfrom kyoka.task import BaseTask\n\nclass MazeTask(BaseTask):\n\n    ACTION_UP = 0\n    ACTION_DOWN = 1\n    ACTION_RIGHT = 2\n    ACTION_LEFT = 3\n\n    # We use current position of the agent in the maze as \nstate\n.\n    # So we return start position of the maze (row=2, col=0).\n    def generate_initial_state(self):\n        return (2, 0)\n\n    # The position of the goal is (row=0, column=8).\n    def is_terminal_state(self):\n        return (0, 8) == state\n\n    # We can always move towards 4 directions.\n    def generate_possible_actions(self, state):\n        return [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_RIGHT, self.ACTION_LEFT]\n\n    # Agent can get reward +1 only when he reaches to the goal.\n    def calculate_reward(self, state):\n        return 1 if self.is_terminal_state(state) else 0\n\n    # Returns next state after moved toward direction of passed action.\n    # If destination is out of the maze or block cell, do not move.\n    def transit_state(self, state, action):\n        row, col = state\n        wall_position = [(1,2), (2,2), (3,2), (4,5), (0,7), (1,7), (2,7)]\n        height, width = 6, 9\n        if self.ACTION_UP == action:\n            row = max(0, row-1)\n        elif self.ACTION_DOWN == action:\n            row = min(height-1, row+1)\n        elif self.ACTION_RIGHT == action:\n            col= min(width-1, col+1)\n        elif self.ACTION_LEFT == action:\n            col = max(0, col-1)\n        if (row, col) not in wall_position:\n            return (row, col)\n        else:\n            return state  # Stay current position if destination is not a path.\n\n\n\n\n\nStep2. Setup QLearning for MazeTask\n\n\nNext we implement \nvalue function\n of our \nMazeTask\n for \nQLearning\n.  \n\n\nvalue function\n is the function which receives \nstate-action\n pair and estimates \nhow good\n for the agent to take the action at the state.\nSo value function would work like this\n\n\nvalue_of_action = value_function.predict_value(state=(1, 5), action=ACTION_UP)\n# value_of_action should be 1 because (1,6) is the goal of the maze.\n\n\n\n\nThe most important part of reinforcement learning is to \nlearn correct value function\n of the task.\n\n\nEach algorithm in this library has different base class of value function\n\n(ex. \nQLearningTabularActionValueFunction\n, \nDeepQLearningApproxActionValueFunction\n).\n\nNow we need to implement abstracted method of \nQLearningTabularActionValueFunction\n.\n\n\nHere is the \nMazeTabularValueFunction\n class for \nQLearning\n.\n\n\nclass MazeTabularValueFunction(QLearningTabularActionValueFunction):\n\n    # We use table(array) to store the value of state-action pair.\n    # Ex. the value of action=ACTION_RIGHT at state=(0,3) is stored in table[0][3][2].\n    def generate_initial_table(self):\n        maze_width, maze_height, action_num = 6, 9, 4\n        return [[[0 for a in range(action_num)] for j in range(width)] for i in range(height)]\n\n    # Define how to fetch value from the table which\n    # initialized by \ngenerate_initial_table\n method.\n    def fetch_value_from_table(self, table, state, action):\n        row, col = state\n        return table[row][col][action]\n\n    # Define how to update the value of table.\n    def insert_value_into_table(self, table, state, action, new_value):\n        row, col = state\n        table[row][col][action] = new_value\n\n\n\n\nFinal Step. Run \nQLearning\n and see its result\n\n\nOk, we prepared everything. Next code starts the learning.\n\n\ntask = MazeTask()\npolicy = EpsilonGreedyPolicy(eps=0.1)\nvalue_function = MazeTabularValueFunction()\nalgorithm = QLearning()\nalgorithm.setup(task, policy, value_function)  # setup before calling \nrun_gpi\n\nalgorithm.run_gpi(nb_iteration=100)  # starts the learning\n\n\n\n\nThat's all !! Now \nvalue_function\n stores how good each action is. Let's visualize what agent learned.\n\n(We prepared helper method \nexamples.maze.helper.visualize_policy\n.)\n\n\n print visualize_policy(task, value_function)\n\n     -------XG\n     --X-v-vX^\nS -\n v-X-vvvX^\n     vvX\n^\n     \n^-^^^\n     -\n^\n^----\n\n\n\n\nGreat!! Agent found the shortest path to the goal. (14 step is the minimum step to the goal !!)\n\n\nInstallation\n\n\nYou can use pip like this.\n\n\npip install kyoka", 
            "title": "Introduction"
        }, 
        {
            "location": "/#kyoka-reinforcement-learning-framework", 
            "text": "", 
            "title": "kyoka - Reinforcement Learning framework"
        }, 
        {
            "location": "/#what-is-reinforcement-learning", 
            "text": "Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. (wikipedia)   In reinforcement learning, the player to learn how to get good result in some task is called as  agent . \nAgent learns which action is good or bad at each situation through large number of simulation. \n(The essential factor to characterize reinforcement learning is  learning from trial-and-error .)", 
            "title": "What is Reinforcement Learning"
        }, 
        {
            "location": "/#why-kyoka-is-creted", 
            "text": "The steps to solve your learning problem (ex. playing Go) by reinforcement learning algorithms would be     Define your learning problem  in Reinforcement Learning format.    Select learning algorithm(ex. QLearning) and  implement it for your learning problem .     We have a lots of things to do before start learning. \nThis library is created to ease implementing these steps.    Sorry, I talked too much. Let's see the code with simple example !!", 
            "title": "Why kyoka is creted"
        }, 
        {
            "location": "/#hello-reinforcement-learning", 
            "text": "We will find the shortest path to escape from below maze by  QLearning .  S: start, G: goal, X: wall\n\n-------XG\n--X----X-\nS-X----X-\n--X------\n-----X---\n---------", 
            "title": "Hello Reinforcement Learning"
        }, 
        {
            "location": "/#step1-define-maze-task", 
            "text": "First we define our learning problem as reinforcement learning task. kyoka  provides  kyoka.task.BaseTask  template class. This class has 5 abstracted methods you need to implement.   gegenerate_inital_state  : define start state of our problem  is_terminal_state  : define when is the finish of our problem  transit_state  : define the rule of state transition in our problem  generate_possible_actions  : define what action is possible in each state  calculate_reward  : define how good each state is   Here is the  MazeTask  class which represents our learning problem.  from kyoka.task import BaseTask\n\nclass MazeTask(BaseTask):\n\n    ACTION_UP = 0\n    ACTION_DOWN = 1\n    ACTION_RIGHT = 2\n    ACTION_LEFT = 3\n\n    # We use current position of the agent in the maze as  state .\n    # So we return start position of the maze (row=2, col=0).\n    def generate_initial_state(self):\n        return (2, 0)\n\n    # The position of the goal is (row=0, column=8).\n    def is_terminal_state(self):\n        return (0, 8) == state\n\n    # We can always move towards 4 directions.\n    def generate_possible_actions(self, state):\n        return [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_RIGHT, self.ACTION_LEFT]\n\n    # Agent can get reward +1 only when he reaches to the goal.\n    def calculate_reward(self, state):\n        return 1 if self.is_terminal_state(state) else 0\n\n    # Returns next state after moved toward direction of passed action.\n    # If destination is out of the maze or block cell, do not move.\n    def transit_state(self, state, action):\n        row, col = state\n        wall_position = [(1,2), (2,2), (3,2), (4,5), (0,7), (1,7), (2,7)]\n        height, width = 6, 9\n        if self.ACTION_UP == action:\n            row = max(0, row-1)\n        elif self.ACTION_DOWN == action:\n            row = min(height-1, row+1)\n        elif self.ACTION_RIGHT == action:\n            col= min(width-1, col+1)\n        elif self.ACTION_LEFT == action:\n            col = max(0, col-1)\n        if (row, col) not in wall_position:\n            return (row, col)\n        else:\n            return state  # Stay current position if destination is not a path.", 
            "title": "Step1. Define Maze Task"
        }, 
        {
            "location": "/#step2-setup-qlearning-for-mazetask", 
            "text": "Next we implement  value function  of our  MazeTask  for  QLearning .    value function  is the function which receives  state-action  pair and estimates  how good  for the agent to take the action at the state.\nSo value function would work like this  value_of_action = value_function.predict_value(state=(1, 5), action=ACTION_UP)\n# value_of_action should be 1 because (1,6) is the goal of the maze.  The most important part of reinforcement learning is to  learn correct value function  of the task.  Each algorithm in this library has different base class of value function \n(ex.  QLearningTabularActionValueFunction ,  DeepQLearningApproxActionValueFunction ). \nNow we need to implement abstracted method of  QLearningTabularActionValueFunction .  Here is the  MazeTabularValueFunction  class for  QLearning .  class MazeTabularValueFunction(QLearningTabularActionValueFunction):\n\n    # We use table(array) to store the value of state-action pair.\n    # Ex. the value of action=ACTION_RIGHT at state=(0,3) is stored in table[0][3][2].\n    def generate_initial_table(self):\n        maze_width, maze_height, action_num = 6, 9, 4\n        return [[[0 for a in range(action_num)] for j in range(width)] for i in range(height)]\n\n    # Define how to fetch value from the table which\n    # initialized by  generate_initial_table  method.\n    def fetch_value_from_table(self, table, state, action):\n        row, col = state\n        return table[row][col][action]\n\n    # Define how to update the value of table.\n    def insert_value_into_table(self, table, state, action, new_value):\n        row, col = state\n        table[row][col][action] = new_value", 
            "title": "Step2. Setup QLearning for MazeTask"
        }, 
        {
            "location": "/#final-step-run-qlearning-and-see-its-result", 
            "text": "Ok, we prepared everything. Next code starts the learning.  task = MazeTask()\npolicy = EpsilonGreedyPolicy(eps=0.1)\nvalue_function = MazeTabularValueFunction()\nalgorithm = QLearning()\nalgorithm.setup(task, policy, value_function)  # setup before calling  run_gpi \nalgorithm.run_gpi(nb_iteration=100)  # starts the learning  That's all !! Now  value_function  stores how good each action is. Let's visualize what agent learned. \n(We prepared helper method  examples.maze.helper.visualize_policy .)   print visualize_policy(task, value_function)\n\n     -------XG\n     --X-v-vX^\nS -  v-X-vvvX^\n     vvX ^\n      ^-^^^\n     - ^ ^----  Great!! Agent found the shortest path to the goal. (14 step is the minimum step to the goal !!)", 
            "title": "Final Step. Run QLearning and see its result"
        }, 
        {
            "location": "/#installation", 
            "text": "You can use pip like this.  pip install kyoka", 
            "title": "Installation"
        }, 
        {
            "location": "/tutorial/tabular_method_tutorial/", 
            "text": "Tabular Reinforcement Learning Problem\n\n\nIn this tutorial, we will solve the problem called \ntabular reinforcement learning problem\n.  \n\n\nThe keyword \ntabular\n means \nstate-action space of the problem is small enough to fit in array or table\n.\n\nMost of reinforcement learning methods have good convergence property on \ntabular reinforcement learning problem\n.\n\nSo first we will approach this type of problem.\n\n\nblocking maze problem\n\n\nWe will find the shortest path of \nblocking maze\n.\n\n\nblocking maze\n transforms its structure during training like below.  \n\n\n  Before         After\n --------G     --------G\n ---------     ---------\n ---------  =\n ---------\n XXXXXXXX-     -XXXXXXXX\n ---------     ---------\n ---S-----     ---S-----\n# 10 step       16 step  \n= minimum step\n\n\n\n\nThe best path in first structure is blocked by transformation.\n\nSo agent needs to realize the transformation and re-learn shortest path.\n\n\nImplementation\n\n\nDefine blocking maze task\n\n\nFirst we define our \nblocking maze problem\n as reinforcement learning task.\nWhat we need to do is implementing 5 abstracted methods of \nBaseTask\n class.\nSo our \nBlockingMazeTask\n would be like this.\n\n\nfrom kyoka.task import BaseTask\n\nclass BlockingMazeTask(BaseTask):\n\n    UP, DOWN, RIGHT, LEFT = 0, 1, 2, 3\n    START, GOAL = (5, 3), (0, 8)\n    BEFORE_MAZE = [\n            ['-','-','-','-','-','-','-','-','G'],\n            ['-','-','-','-','-','-','-','-','-'],\n            ['-','-','-','-','-','-','-','-','-'],\n            ['X','X','X','X','X','X','X','X','-'],\n            ['-','-','-','-','-','-','-','-','-'],\n            ['-','-','-','S','-','-','-','-','-']\n    ]\n    AFTER_MAZE = [\n            ['-','-','-','-','-','-','-','-','G'],\n            ['-','-','-','-','-','-','-','-','-'],\n            ['-','-','-','-','-','-','-','-','-'],\n            ['-','X','X','X','X','X','X','X','X'],\n            ['-','-','-','-','-','-','-','-','-'],\n            ['-','-','-','S','-','-','-','-','-']\n    ]\n\n    def __init__(self):\n        self.maze = self.BEFORE_MAZE\n\n    def generate_initial_state(self):\n        return self.START\n\n    def is_terminal_state(self, state):\n        return self.GOAL == state\n\n    def transit_state(self, state, action):\n        row, col = state\n        height, width = 6, 9\n        if action == self.UP:\n            row = max(0, row-1)\n        elif action == self.DOWN:\n            row = min(height-1, row+1)\n        elif action == self.RIGHT:\n            col= min(width-1, col+1)\n        elif action == self.LEFT:\n            col = max(0, col-1)\n        if 'X' != self.maze[row][col]:\n            return (row, col)\n        else:\n            return state\n\n    def generate_possible_actions(self, state):\n        return [self.UP, self.DOWN, self.RIGHT, self.LEFT]\n\n    def calculate_reward(self, state):\n        row, col = state\n        return 1 if 'G' == self.maze[row][col] else 0\n\n\n\n\nThis code does not imeplment transformation feature yet.\n\nWe implement the transformation feature by using \nkyoka.callback\n module later.\n\n\nSetup algorithm for blocking maze problem\n\n\nNext we create \nvalue function\n of our task.\n\n\n\n\nvalue function is the function which receives state-action pair and estimates how good for the agent to take the action at the state.\n\n\n\n\nBefore start implementation, let's think about the \nsize of state-action space\n of our blocking maze problem.\n\nIn our \nBlockingMazeTask\n, state and action is defined like this.\n\n\n\n\nstate  = position of agent in the maze \n\n\naction = the direction to move (UP or DOWN or RIGHT or LEFT)\n\n\n\n\nThe number of possible state is \n6*9=54\n(our maze shape is 6x9) and we can move \n4\n direction in every state.\nSo the \nsize of state-action space\n is \n54*4=216\n.\n\nThis indicates that our learning problem is \nsmall enough to fit in array or table\n.\n\nSo we can say that blocking maze problem is \ntabular reinforcement learning problem\n.\n\n\nOk, let's resume implementation.\nHere we use \nSarsa\n method to find shortest path.\n\nWhat we need to do is implementing abstract methods of \nSarsaTabularActionValueFunction\n like this.\n\n\nclass MazeTabularValueFunction(SarsaTabularActionValueFunction):\n\n    def generate_initial_table(self):\n        maze_width, maze_height, action_num = 6, 9, 4\n        return [[[0 for a in range(action_num)] for j in range(width)] for i in range(height)]\n\n    def fetch_value_from_table(self, table, state, action):\n        row, col = state\n        return table[row][col][action]\n\n    def insert_value_into_table(self, table, state, action, new_value):\n        row, col = state\n        table[row][col][action] = new_value\n\n\n\n\nImplement maze transformation feature\n\n\nWe implement maze transformation feature by using \nkeras.callback\n module.\nThis module provides the callback methods to interact with our task and value function under training.  \n\n\nBase class of all callback is \nkeras.callback.BaseCallback\n. This class has 4 callback methods.\n\n\n\n\nbefore_gpi_start(task, value_function)\n\n\nbefore_update(iteration_count, task, value_function)\n\n\nafter_update(iteration_count, task, value_function)\n\n\nafter_gpi_finish(task, value_function)\n\n\n\n\nHere we create the \nMazeTransformationCallback\n which interacts with \nBlockingMazeTask\n after 50 iteration of training and switch the shape of maze. The code is like this.\n\n\nfrom kyoka.callback import BaseCallback\n\nclass MazeTransformationCallback(BaseCallback):\n\n    def after_update(self, iteration_count, task, value_function):\n        if 50 == iteration_count:\n            task.maze = BlockingMazeTask.AFTER_MAZE\n            # we recommend you to use \nself.log(message)\n instead of \nprint\n method in callback.\n            self.log(\nMaze transformed after %d iteration.\n % iteration_count)\n\n\n\n\n\nWatch performance of agent during training\n\n\nBefore start training, we implement one more important callback \nMazePerformanceWatcher\n.\n\nThis callback logs performance of agent in each iteration of training.\n\n(In our case, performance means \nhow many step does agent takes to the goal\n.)\n\n\nWith \ntask\n and \nvalue_function\n, we can test performance of agent like this.\n\n\nfrom kyoka.policy import choose_best_action\n\nMAX_STEP = 10000\ndef solve_maze(task, value_function):\n    step_count = 0\n    state = task.generate_initial_state()\n    while not task.is_terminal_state():\n        action = choose_best_action(task, value_function, state)\n        state = task.transit_state(state, action)\n        step_count += 1\n        if step_count \n= MAX_STEP:  # agent may never reaches to the goal\n            break\n    return step_count\n\n\n\n\n\nWe create \nMazePerformanceWatcher\n by using \nkyoka.callback.BasePerformanceWatcher\n callback.\n\nThe code would be like this.\n\n\nclass MazePerformanceWatcher(BasePerformanceWatcher):\n\n    def define_performance_test_interval(self):\n        return 1  # this means \nrun_performance_test\n is called in every \nafter_update\n callback\n\n    # Do performance test and return its result.\n    # Result is passed to \ndefine_log_message\n method as \ntest_result\n argument.\n    def run_performance_test(self, task, value_function):\n        step_to_goal = solve_maze(task, value_function)\n        return step_to_goal\n\n    # The message returned here is logged after every \nrun_performance_test\n is called.\n    def define_log_message(self, iteration_count, task, value_function, test_result):\n        step_to_goal = test_result\n        return \nStep = %d (iteration=%d)\n % (step_to_goal,iteration_count)\n\n\n\n\n\nSolve blocking maze\n\n\nOk, we prepared everything to start training.  Let's start !!\n\n\nfrom kyoka.algorithm.sarsa import Sarsa\nfrom kyoka.policy import EpsilonGreedyPolicy\n\ntask = BlockingMazeTask()\npolicy = EpsilonGreedyPolicy(eps=0.1)\nvalue_function = MazeTabularValueFunction()\nalgorithm = Sarsa()\nalgorithm.setup(task, policy, value_function)\n\ncallbacks = [MazeTransformationCallback(), MazePerformanceWatcher()]\nalgorithm.run_gpi(nb_iteration=100, callbacks=callbacks)\n\n\n\n\nTraining logs would be output on console like this\n\n\n[Progress] Start GPI iteration for 100 times\n[Progress] Finished 1 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 223 (nb_iteration=1)\n[Progress] Finished 2 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 507 (nb_iteration=2)\n[Progress] Finished 3 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 220 (nb_iteration=3)\n[Progress] Finished 4 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 142 (nb_iteration=4)\n[Progress] Finished 5 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 572 (nb_iteration=5)\n[Progress] Finished 6 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 781 (nb_iteration=6)\n[Progress] Finished 7 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 18 (nb_iteration=7)\n...\n[Progress] Finished 50 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 10 (nb_iteration=50)\n[MazeTransformationCallback] Maze transformed after 50 iteration\n[Progress] Finished 51 / 100 iterations (1.6s)\n[MazePerformanceWatcher] Step = 10000 (nb_iteration=51)\n[Progress] Finished 52 / 100 iterations (1.4s)\n[MazePerformanceWatcher] Step = 10000 (nb_iteration=52)\n...\nProgress] Finished 99 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 16 (nb_iteration=99)\n[Progress] Finished 100 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 16 (nb_iteration=100)\n[Progress] Completed GPI iteration for 100 times. (total time: 7s)\n\n\n\n\nGreat!! Agent found the shortest path of blocking maze before and after transofmation.", 
            "title": "Tabular Method Tutorial"
        }, 
        {
            "location": "/tutorial/tabular_method_tutorial/#tabular-reinforcement-learning-problem", 
            "text": "In this tutorial, we will solve the problem called  tabular reinforcement learning problem .    The keyword  tabular  means  state-action space of the problem is small enough to fit in array or table . \nMost of reinforcement learning methods have good convergence property on  tabular reinforcement learning problem . \nSo first we will approach this type of problem.", 
            "title": "Tabular Reinforcement Learning Problem"
        }, 
        {
            "location": "/tutorial/tabular_method_tutorial/#blocking-maze-problem", 
            "text": "We will find the shortest path of  blocking maze .  blocking maze  transforms its structure during training like below.      Before         After\n --------G     --------G\n ---------     ---------\n ---------  =  ---------\n XXXXXXXX-     -XXXXXXXX\n ---------     ---------\n ---S-----     ---S-----\n# 10 step       16 step   = minimum step  The best path in first structure is blocked by transformation. \nSo agent needs to realize the transformation and re-learn shortest path.", 
            "title": "blocking maze problem"
        }, 
        {
            "location": "/tutorial/tabular_method_tutorial/#implementation", 
            "text": "", 
            "title": "Implementation"
        }, 
        {
            "location": "/tutorial/tabular_method_tutorial/#define-blocking-maze-task", 
            "text": "First we define our  blocking maze problem  as reinforcement learning task.\nWhat we need to do is implementing 5 abstracted methods of  BaseTask  class.\nSo our  BlockingMazeTask  would be like this.  from kyoka.task import BaseTask\n\nclass BlockingMazeTask(BaseTask):\n\n    UP, DOWN, RIGHT, LEFT = 0, 1, 2, 3\n    START, GOAL = (5, 3), (0, 8)\n    BEFORE_MAZE = [\n            ['-','-','-','-','-','-','-','-','G'],\n            ['-','-','-','-','-','-','-','-','-'],\n            ['-','-','-','-','-','-','-','-','-'],\n            ['X','X','X','X','X','X','X','X','-'],\n            ['-','-','-','-','-','-','-','-','-'],\n            ['-','-','-','S','-','-','-','-','-']\n    ]\n    AFTER_MAZE = [\n            ['-','-','-','-','-','-','-','-','G'],\n            ['-','-','-','-','-','-','-','-','-'],\n            ['-','-','-','-','-','-','-','-','-'],\n            ['-','X','X','X','X','X','X','X','X'],\n            ['-','-','-','-','-','-','-','-','-'],\n            ['-','-','-','S','-','-','-','-','-']\n    ]\n\n    def __init__(self):\n        self.maze = self.BEFORE_MAZE\n\n    def generate_initial_state(self):\n        return self.START\n\n    def is_terminal_state(self, state):\n        return self.GOAL == state\n\n    def transit_state(self, state, action):\n        row, col = state\n        height, width = 6, 9\n        if action == self.UP:\n            row = max(0, row-1)\n        elif action == self.DOWN:\n            row = min(height-1, row+1)\n        elif action == self.RIGHT:\n            col= min(width-1, col+1)\n        elif action == self.LEFT:\n            col = max(0, col-1)\n        if 'X' != self.maze[row][col]:\n            return (row, col)\n        else:\n            return state\n\n    def generate_possible_actions(self, state):\n        return [self.UP, self.DOWN, self.RIGHT, self.LEFT]\n\n    def calculate_reward(self, state):\n        row, col = state\n        return 1 if 'G' == self.maze[row][col] else 0  This code does not imeplment transformation feature yet. \nWe implement the transformation feature by using  kyoka.callback  module later.", 
            "title": "Define blocking maze task"
        }, 
        {
            "location": "/tutorial/tabular_method_tutorial/#setup-algorithm-for-blocking-maze-problem", 
            "text": "Next we create  value function  of our task.   value function is the function which receives state-action pair and estimates how good for the agent to take the action at the state.   Before start implementation, let's think about the  size of state-action space  of our blocking maze problem. \nIn our  BlockingMazeTask , state and action is defined like this.   state  = position of agent in the maze   action = the direction to move (UP or DOWN or RIGHT or LEFT)   The number of possible state is  6*9=54 (our maze shape is 6x9) and we can move  4  direction in every state.\nSo the  size of state-action space  is  54*4=216 . \nThis indicates that our learning problem is  small enough to fit in array or table . \nSo we can say that blocking maze problem is  tabular reinforcement learning problem .  Ok, let's resume implementation.\nHere we use  Sarsa  method to find shortest path. \nWhat we need to do is implementing abstract methods of  SarsaTabularActionValueFunction  like this.  class MazeTabularValueFunction(SarsaTabularActionValueFunction):\n\n    def generate_initial_table(self):\n        maze_width, maze_height, action_num = 6, 9, 4\n        return [[[0 for a in range(action_num)] for j in range(width)] for i in range(height)]\n\n    def fetch_value_from_table(self, table, state, action):\n        row, col = state\n        return table[row][col][action]\n\n    def insert_value_into_table(self, table, state, action, new_value):\n        row, col = state\n        table[row][col][action] = new_value", 
            "title": "Setup algorithm for blocking maze problem"
        }, 
        {
            "location": "/tutorial/tabular_method_tutorial/#implement-maze-transformation-feature", 
            "text": "We implement maze transformation feature by using  keras.callback  module.\nThis module provides the callback methods to interact with our task and value function under training.    Base class of all callback is  keras.callback.BaseCallback . This class has 4 callback methods.   before_gpi_start(task, value_function)  before_update(iteration_count, task, value_function)  after_update(iteration_count, task, value_function)  after_gpi_finish(task, value_function)   Here we create the  MazeTransformationCallback  which interacts with  BlockingMazeTask  after 50 iteration of training and switch the shape of maze. The code is like this.  from kyoka.callback import BaseCallback\n\nclass MazeTransformationCallback(BaseCallback):\n\n    def after_update(self, iteration_count, task, value_function):\n        if 50 == iteration_count:\n            task.maze = BlockingMazeTask.AFTER_MAZE\n            # we recommend you to use  self.log(message)  instead of  print  method in callback.\n            self.log( Maze transformed after %d iteration.  % iteration_count)", 
            "title": "Implement maze transformation feature"
        }, 
        {
            "location": "/tutorial/tabular_method_tutorial/#watch-performance-of-agent-during-training", 
            "text": "Before start training, we implement one more important callback  MazePerformanceWatcher . \nThis callback logs performance of agent in each iteration of training. \n(In our case, performance means  how many step does agent takes to the goal .)  With  task  and  value_function , we can test performance of agent like this.  from kyoka.policy import choose_best_action\n\nMAX_STEP = 10000\ndef solve_maze(task, value_function):\n    step_count = 0\n    state = task.generate_initial_state()\n    while not task.is_terminal_state():\n        action = choose_best_action(task, value_function, state)\n        state = task.transit_state(state, action)\n        step_count += 1\n        if step_count  = MAX_STEP:  # agent may never reaches to the goal\n            break\n    return step_count  We create  MazePerformanceWatcher  by using  kyoka.callback.BasePerformanceWatcher  callback. \nThe code would be like this.  class MazePerformanceWatcher(BasePerformanceWatcher):\n\n    def define_performance_test_interval(self):\n        return 1  # this means  run_performance_test  is called in every  after_update  callback\n\n    # Do performance test and return its result.\n    # Result is passed to  define_log_message  method as  test_result  argument.\n    def run_performance_test(self, task, value_function):\n        step_to_goal = solve_maze(task, value_function)\n        return step_to_goal\n\n    # The message returned here is logged after every  run_performance_test  is called.\n    def define_log_message(self, iteration_count, task, value_function, test_result):\n        step_to_goal = test_result\n        return  Step = %d (iteration=%d)  % (step_to_goal,iteration_count)", 
            "title": "Watch performance of agent during training"
        }, 
        {
            "location": "/tutorial/tabular_method_tutorial/#solve-blocking-maze", 
            "text": "Ok, we prepared everything to start training.  Let's start !!  from kyoka.algorithm.sarsa import Sarsa\nfrom kyoka.policy import EpsilonGreedyPolicy\n\ntask = BlockingMazeTask()\npolicy = EpsilonGreedyPolicy(eps=0.1)\nvalue_function = MazeTabularValueFunction()\nalgorithm = Sarsa()\nalgorithm.setup(task, policy, value_function)\n\ncallbacks = [MazeTransformationCallback(), MazePerformanceWatcher()]\nalgorithm.run_gpi(nb_iteration=100, callbacks=callbacks)  Training logs would be output on console like this  [Progress] Start GPI iteration for 100 times\n[Progress] Finished 1 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 223 (nb_iteration=1)\n[Progress] Finished 2 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 507 (nb_iteration=2)\n[Progress] Finished 3 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 220 (nb_iteration=3)\n[Progress] Finished 4 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 142 (nb_iteration=4)\n[Progress] Finished 5 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 572 (nb_iteration=5)\n[Progress] Finished 6 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 781 (nb_iteration=6)\n[Progress] Finished 7 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 18 (nb_iteration=7)\n...\n[Progress] Finished 50 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 10 (nb_iteration=50)\n[MazeTransformationCallback] Maze transformed after 50 iteration\n[Progress] Finished 51 / 100 iterations (1.6s)\n[MazePerformanceWatcher] Step = 10000 (nb_iteration=51)\n[Progress] Finished 52 / 100 iterations (1.4s)\n[MazePerformanceWatcher] Step = 10000 (nb_iteration=52)\n...\nProgress] Finished 99 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 16 (nb_iteration=99)\n[Progress] Finished 100 / 100 iterations (0.0s)\n[MazePerformanceWatcher] Step = 16 (nb_iteration=100)\n[Progress] Completed GPI iteration for 100 times. (total time: 7s)  Great!! Agent found the shortest path of blocking maze before and after transofmation.", 
            "title": "Solve blocking maze"
        }, 
        {
            "location": "/tutorial/approximation_method_tutorial/", 
            "text": "Reinforcement Learning with Approximation\n\n\nSorry, under the construction...", 
            "title": "Approximation Method Tutorial"
        }, 
        {
            "location": "/tutorial/approximation_method_tutorial/#reinforcement-learning-with-approximation", 
            "text": "Sorry, under the construction...", 
            "title": "Reinforcement Learning with Approximation"
        }, 
        {
            "location": "/algorithm/about_algorithms/", 
            "text": "About Reinforcement Learning Algorithms\n\n\nAll reinforcement learning algorithms implemented in this library have these methods in common\n\n\nalgorithm.setup(task, policy value_function)\n\n\nYou must need to call this method before start training.\n  \n\n\nThis method sets passed \ntask\n, \npolicy\n, \nvalue_function\n on the algorithm.\n\nSo you can access these items by \nalgorithm.task\n, \nalgorithm.policy\n, \nalgorithm.value_function\n.\n\nThe \nsetup\n method of value function is also called in this method.  \n\n\n\n\nalgorithm.run_gpi(nb_iteration, callbacks=None, verbose=1)\n\n\nThis method starts training of value function with items passed in \nsetup\n method.  \n\n\n\n\nnb_iteration\n is the number of episodes of your task played in the training.  \n\n\nYou can also set \ncallbacks\n objects to interact with task or value function during training.  \n\n\nIf you want to suppress log of training progress set \nverbose=0\n\n\n\n\n\n\nalgorithm.save(save_dir_path)\n, \nalgorithm.load(load_dir_path)\n\n\nYou can save and load training results with these methods.\n\n\nSAVE_DIR_PATH = \n~/dev/rl/my_training_result\n\n\nalgorithm1 = # setup fresh algorithm\n# setup training items...\nalgorithm1.run_gpi(10000)\nalgorithm1.save(SAVE_DIR_PATH)\n\nalgorithm2 = # setup fresh algorithm again\n# setup training items again...\nalgorithm2.load(SAVE_DIR_PATH)\ntrained_value_function = algorithm2.value_function\n\n\n\n\n\n\nThe training code would be like this.\n\n\nSAVE_DIR_PATH = \n~/dev/rl/my_training_result\n\n\ntask = # Instantiate your learning task object\npolicy = EpsilonGreedyPolicy(eps=0.9)\npolicy.set_eps_annealing(initial_eps=0.9, final_eps=0.1, anneal_duration=10000)\n\nalgorithm = # Instantiate some algorithm ex. QLearning()\nvalue_function = # Instantiate your value function\n\ncallbacks = []\n# setup callback objects...\n\n# Do not forget this method before calling \nrun_gpi\n\nalgorithm.setup(task, policy value_function)\n\n# Load last training result if exists\nif os.path.exists(SAVE_DIR_PATH):\n    algorithm.load(SAVE_DIR_PATH)\n\n# Start training of value function for 100000 episode\nalgorithm.run_gpi(100000, callbacks)\n\n# Save training results\nalgorithm.save(SAVE_DIR_PATH)", 
            "title": "About Algorithms"
        }, 
        {
            "location": "/algorithm/about_algorithms/#about-reinforcement-learning-algorithms", 
            "text": "All reinforcement learning algorithms implemented in this library have these methods in common", 
            "title": "About Reinforcement Learning Algorithms"
        }, 
        {
            "location": "/algorithm/about_algorithms/#algorithmsetuptask-policy-value_function", 
            "text": "You must need to call this method before start training.     This method sets passed  task ,  policy ,  value_function  on the algorithm. \nSo you can access these items by  algorithm.task ,  algorithm.policy ,  algorithm.value_function . \nThe  setup  method of value function is also called in this method.", 
            "title": "algorithm.setup(task, policy value_function)"
        }, 
        {
            "location": "/algorithm/about_algorithms/#algorithmrun_gpinb_iteration-callbacksnone-verbose1", 
            "text": "This method starts training of value function with items passed in  setup  method.     nb_iteration  is the number of episodes of your task played in the training.    You can also set  callbacks  objects to interact with task or value function during training.    If you want to suppress log of training progress set  verbose=0", 
            "title": "algorithm.run_gpi(nb_iteration, callbacks=None, verbose=1)"
        }, 
        {
            "location": "/algorithm/about_algorithms/#algorithmsavesave_dir_path-algorithmloadload_dir_path", 
            "text": "You can save and load training results with these methods.  SAVE_DIR_PATH =  ~/dev/rl/my_training_result \n\nalgorithm1 = # setup fresh algorithm\n# setup training items...\nalgorithm1.run_gpi(10000)\nalgorithm1.save(SAVE_DIR_PATH)\n\nalgorithm2 = # setup fresh algorithm again\n# setup training items again...\nalgorithm2.load(SAVE_DIR_PATH)\ntrained_value_function = algorithm2.value_function   The training code would be like this.  SAVE_DIR_PATH =  ~/dev/rl/my_training_result \n\ntask = # Instantiate your learning task object\npolicy = EpsilonGreedyPolicy(eps=0.9)\npolicy.set_eps_annealing(initial_eps=0.9, final_eps=0.1, anneal_duration=10000)\n\nalgorithm = # Instantiate some algorithm ex. QLearning()\nvalue_function = # Instantiate your value function\n\ncallbacks = []\n# setup callback objects...\n\n# Do not forget this method before calling  run_gpi \nalgorithm.setup(task, policy value_function)\n\n# Load last training result if exists\nif os.path.exists(SAVE_DIR_PATH):\n    algorithm.load(SAVE_DIR_PATH)\n\n# Start training of value function for 100000 episode\nalgorithm.run_gpi(100000, callbacks)\n\n# Save training results\nalgorithm.save(SAVE_DIR_PATH)", 
            "title": "algorithm.save(save_dir_path), algorithm.load(load_dir_path)"
        }, 
        {
            "location": "/algorithm/montecarlo/", 
            "text": "MonteCarlo Method\n\n\nMonteCarlo method estimates the value of action by averaging result of random simulation.  \n\n\nAlgorithm\n\n\nOur implementation of MonteCarlo method is the one called as \nevery-visit MonteCarlo method\n.\n\n\nevery-visit\n means \nusing every state for update in an episode even if same state appeared in the episode\n.\n\n\nParameter:\n    g  \n- gamma. discounting factor for reward. [0,1]\nInitialize:\n    T  \n- your RL task\n    PI \n- Policy used to generate episode\n    Q  \n- action value function\n\nRepeat until computational budget runs out:\n    generate an episode of T by following policy PI\n    for each state-action pair (S, A)  appeared in the episode:\n        G \n- sum of rewards gained after state S (G is discounted if g \n 1)\n        Q(S, A) \n- average G of S sampled ever\n\n\n\n\nReward discounting\n\n\nWe support \nreward discounting\n feature.\n\nIf you want to use this feature, set parameter \ngamma\n in range [0, 1).\n\n(\ngamma\n is set to 1 as default value. This means no discounting.)\n\n\nmontecarlo = MonteCarlo(gamma=0.99)\n\n\n\n\nOk, let's see how \nreward discounting\n works.\n\n\nHere we assume MonteCarlo method gets the episode of some task like below\n\n\nstep0. agent at initial state \ns0\n\nstep1. agent take action \na0\n at \ns0\n and move to next state \ns1\n and received reward 0.\nstep2. agent take action \na1\n at \ns1\n and move to next state \ns2\n and received reward 0.\nstep3. agent take action \na2\n at \ns2\n and move to terminal state \ns3\n and received reward 1.\n\n\n\n\nThe value of G at s0 (sum of rewards gained after s0) \nwithout reward discounting\n is\n\n\nG = reward_at_step1 + reward_at_step2 + reward_at_step3\n  = 0 + 0 + 1\n  = 1\n\n\n\n\nThe value of G at s0 \nwith reward discounting\n (if \ngamma=0.99\n) is\n\n\nG = reward_at_step1 + gamma * reward_at_step2 + gamma**2 * reward_at_step3\n  = 0 + 0.1 * 0 + 0.1**2 * 1\n  = 0.9801\n\n\n\n\nValue function\n\n\nMonteCarlo method provides \ntabular\n and \napproximation\n type of value functions.  \n\n\nMonteCarloTabularActionValueFunction\n\n\nIf your task is \ntabular size\n, you can use \nMonteCarloTabularActionValueFunction\n.\n\n\n\n\nIf you can store the value of all state-action pair on the memory(array), your task is \ntabular\n size.\n\n\n\n\nMonteCarloTabularActionValueFunction\n has 3 abstracted method to define the table size of your task.\n\n\n\n\ngenerate_initial_table\n : initialize table object and return it here\n\n\nfetch_value_from_table\n : define how to fetch value from your table\n\n\ninsert_value_into_table\n : define how to insert new value into your table\n\n\n\n\nIf the shape of your state-action space is SxA, implementation would be like this.\n\n\nclass MyTabularActionValueFunction(MonteCarloTabularActionValueFunction):\n\n    def generate_initial_table(self):\n        return [[0 for j in range(A)] for i in range(S)]\n\n    def fetch_value_from_table(self, table, state, action):\n        return table[state][action]\n\n    def insert_value_into_table(self, table, state, action, new_value):\n        table[state][action] = new_value\n\n\n\n\nMonteCarloApproxActionValueFunction\n\n\nIf your task is not \ntabular\n size, you use \nMonteCarloApproxActionValueFunction\n.\n\n\nMonteCarloApproxActionValueFunction\n has 3 abstracted methods. You would wrap some prediction model (ex. neuralnet) in these methods.\n\n\n\n\nconstruct_features\n : transform state-action pair into feature representation\n\n\napprox_predict_value\n : predict value of state-action pair with prediction model you want to use\n\n\napprox_backup\n : update your model in supervised learning way with passed input and output pair\n\n\n\n\nThe implementation with some neuralnet library would be like this.\n\n\nclass MyApproxActionValueFunction(MonteCarloApproxActionValueFunction):\n\n    def setup(self):\n        super(MazeApproxActionValueFunction, self).setup()\n        self.neuralnet = build_neuralnet_in_some_way()\n\n    def construct_features(self, state, action):\n        feature1 = do_something(state, action)\n        feature2 = do_anotherthing(state, action)\n        return [feature1, feature2]\n\n    def approx_predict_value(self, features):\n        return self.neuralnet.predict(features)\n\n    def approx_backup(self, features, backup_target, alpha):\n        self.neuralnet.incremental_training(X=features, Y=backup_target)\n\n\n\n\nSample code to start learning\n\n\ntest_length = 1000\ntask = MyTask()\npolicy = EpsilonGreedyPolicy(eps=0.1)\nvalue_func = MyTabularActionValueFunction()\nalgorithm = MonteCarlo(gamma=0.99)\nalgorithm.setup(task, policy, value_func)\nalgorithm.run_gpi(test_length)", 
            "title": "MonteCarlo"
        }, 
        {
            "location": "/algorithm/montecarlo/#montecarlo-method", 
            "text": "MonteCarlo method estimates the value of action by averaging result of random simulation.", 
            "title": "MonteCarlo Method"
        }, 
        {
            "location": "/algorithm/montecarlo/#algorithm", 
            "text": "Our implementation of MonteCarlo method is the one called as  every-visit MonteCarlo method .  every-visit  means  using every state for update in an episode even if same state appeared in the episode .  Parameter:\n    g   - gamma. discounting factor for reward. [0,1]\nInitialize:\n    T   - your RL task\n    PI  - Policy used to generate episode\n    Q   - action value function\n\nRepeat until computational budget runs out:\n    generate an episode of T by following policy PI\n    for each state-action pair (S, A)  appeared in the episode:\n        G  - sum of rewards gained after state S (G is discounted if g   1)\n        Q(S, A)  - average G of S sampled ever", 
            "title": "Algorithm"
        }, 
        {
            "location": "/algorithm/montecarlo/#reward-discounting", 
            "text": "We support  reward discounting  feature. \nIf you want to use this feature, set parameter  gamma  in range [0, 1). \n( gamma  is set to 1 as default value. This means no discounting.)  montecarlo = MonteCarlo(gamma=0.99)  Ok, let's see how  reward discounting  works.  Here we assume MonteCarlo method gets the episode of some task like below  step0. agent at initial state  s0 \nstep1. agent take action  a0  at  s0  and move to next state  s1  and received reward 0.\nstep2. agent take action  a1  at  s1  and move to next state  s2  and received reward 0.\nstep3. agent take action  a2  at  s2  and move to terminal state  s3  and received reward 1.  The value of G at s0 (sum of rewards gained after s0)  without reward discounting  is  G = reward_at_step1 + reward_at_step2 + reward_at_step3\n  = 0 + 0 + 1\n  = 1  The value of G at s0  with reward discounting  (if  gamma=0.99 ) is  G = reward_at_step1 + gamma * reward_at_step2 + gamma**2 * reward_at_step3\n  = 0 + 0.1 * 0 + 0.1**2 * 1\n  = 0.9801", 
            "title": "Reward discounting"
        }, 
        {
            "location": "/algorithm/montecarlo/#value-function", 
            "text": "MonteCarlo method provides  tabular  and  approximation  type of value functions.", 
            "title": "Value function"
        }, 
        {
            "location": "/algorithm/montecarlo/#montecarlotabularactionvaluefunction", 
            "text": "If your task is  tabular size , you can use  MonteCarloTabularActionValueFunction .   If you can store the value of all state-action pair on the memory(array), your task is  tabular  size.   MonteCarloTabularActionValueFunction  has 3 abstracted method to define the table size of your task.   generate_initial_table  : initialize table object and return it here  fetch_value_from_table  : define how to fetch value from your table  insert_value_into_table  : define how to insert new value into your table   If the shape of your state-action space is SxA, implementation would be like this.  class MyTabularActionValueFunction(MonteCarloTabularActionValueFunction):\n\n    def generate_initial_table(self):\n        return [[0 for j in range(A)] for i in range(S)]\n\n    def fetch_value_from_table(self, table, state, action):\n        return table[state][action]\n\n    def insert_value_into_table(self, table, state, action, new_value):\n        table[state][action] = new_value", 
            "title": "MonteCarloTabularActionValueFunction"
        }, 
        {
            "location": "/algorithm/montecarlo/#montecarloapproxactionvaluefunction", 
            "text": "If your task is not  tabular  size, you use  MonteCarloApproxActionValueFunction .  MonteCarloApproxActionValueFunction  has 3 abstracted methods. You would wrap some prediction model (ex. neuralnet) in these methods.   construct_features  : transform state-action pair into feature representation  approx_predict_value  : predict value of state-action pair with prediction model you want to use  approx_backup  : update your model in supervised learning way with passed input and output pair   The implementation with some neuralnet library would be like this.  class MyApproxActionValueFunction(MonteCarloApproxActionValueFunction):\n\n    def setup(self):\n        super(MazeApproxActionValueFunction, self).setup()\n        self.neuralnet = build_neuralnet_in_some_way()\n\n    def construct_features(self, state, action):\n        feature1 = do_something(state, action)\n        feature2 = do_anotherthing(state, action)\n        return [feature1, feature2]\n\n    def approx_predict_value(self, features):\n        return self.neuralnet.predict(features)\n\n    def approx_backup(self, features, backup_target, alpha):\n        self.neuralnet.incremental_training(X=features, Y=backup_target)", 
            "title": "MonteCarloApproxActionValueFunction"
        }, 
        {
            "location": "/algorithm/montecarlo/#sample-code-to-start-learning", 
            "text": "test_length = 1000\ntask = MyTask()\npolicy = EpsilonGreedyPolicy(eps=0.1)\nvalue_func = MyTabularActionValueFunction()\nalgorithm = MonteCarlo(gamma=0.99)\nalgorithm.setup(task, policy, value_func)\nalgorithm.run_gpi(test_length)", 
            "title": "Sample code to start learning"
        }, 
        {
            "location": "/algorithm/sarsa/", 
            "text": "Sarsa - on-policy TD learning method\n\n\nSarsa method updates value of state-action pair \nQ(s,a)\nin following way.\n\n\ns  : current state\na  : action to take at state s choosed by policy PI\nr  : reward by transition (s, a)\ns' : next state after took action a at s\na' : action to take at state s' choosed by policy PI\n\nQ(s,a) = Q(s,a) + alpha [ r + gamma * Q(s', a') - Q(s, a) ]\n\n\n\n\nUpdate is done with variable \ns,a,r,s',a'\n. So this algorithm is named as Sarsa.\n\n\nThis method is also called as \non-policy TD learning\n method.  The keyword is \non-policy\n.\n\n\non-policy\n means that Sarsa uses same policy PI to calculate \na\n and \na'\n.\n\nThe algorithm uses different policy to select \na\n and \na'\n is called \noff-policy\n method. (ex. QLearning)\n\n\nAlgorithm\n\n\nParameter:\n    a  \n- alpha. learning rate. [0,1].\n    g  \n- gamma. discounting factor. [0,1].\nInitialize:\n    T  \n- your RL task\n    PI \n- policy used in the algorithm\n    Q  \n- action value function\n\n    Repeat until computational budget runs out:\n        S \n- generate initial state of task T\n        A \n- choose action at S by following policy PI\n        Repeat until S is terminal state:\n            S' \n- next state of S after taking action A\n            R \n- reward gained by taking action A at state S\n            A' \n- next action at S' by following policy PI\n            Q(S, A) \n- Q(S, A) + a * [ R + g * Q(S', A') - Q(S, A)]\n            S, A \n- S', A'\n\n\n\n\nValue function\n\n\nSarsa method provides \ntabular\n and \napproximation\n type of value functions.\n\n\nSarsaTabularActionValueFunction\n\n\nIf your task is \ntabular size\n, you can use \nSarsaTabularActionValueFunction\n.\n\n\n\n\nIf you can store the value of all state-action pair on the memory(array), your task is \ntabular\n size.\n\n\n\n\nSarsaTabularActionValueFunction\n has 3 abstracted method to define the table size of your task.\n\n\n\n\ngenerate_initial_table\n : initialize table object and return it here\n\n\nfetch_value_from_table\n : define how to fetch value from your table\n\n\ninsert_value_into_table\n : define how to insert new value into your table\n\n\n\n\nIf the shape of your state-action space is SxA, implementation would be like this.\n\n\nclass MyTabularActionValueFunction(SarsaTabularActionValueFunction):\n\n    def generate_initial_table(self):\n        return [[0 for j in range(A)] for i in range(S)]\n\n    def fetch_value_from_table(self, table, state, action):\n        return table[state][action]\n\n    def insert_value_into_table(self, table, state, action, new_value):\n        table[state][action] = new_value\n\n\n\n\nSarsaApproxActionValueFunction\n\n\nIf your task is not \ntabular\n size, you use \nSarsaApproxActionValueFunction\n.\n\n\nSarsaApproxActionValueFunction\n has 3 abstracted methods. You would wrap some prediction model (ex. neuralnet) in these methods.\n\n\n\n\nconstruct_features\n : transform state-action pair into feature representation\n\n\napprox_predict_value\n : predict value of state-action pair with prediction model you want to use\n\n\napprox_backup\n : update your model in supervised learning way with passed input and output pair\n\n\n\n\nThe implementation with some neuralnet library would be like this.\n\n\nclass MyApproxActionValueFunction(SarsaApproxActionValueFunction):\n\n    def setup(self):\n        super(MazeApproxActionValueFunction, self).setup()\n        self.neuralnet = build_neuralnet_in_some_way()\n\n    def construct_features(self, state, action):\n        feature1 = do_something(state, action)\n        feature2 = do_anotherthing(state, action)\n        return [feature1, feature2]\n\n    def approx_predict_value(self, features):\n        return self.neuralnet.predict(features)\n\n    def approx_backup(self, features, backup_target, alpha):\n        self.neuralnet.incremental_training(X=features, Y=backup_target)\n\n\n\n\nSample code to start learning\n\n\ntest_length = 1000\ntask = MyTask()\npolicy = EpsilonGreedyPolicy(eps=0.1)\nvalue_func = MyTabularActionValueFunction()\nalgorithm = Sarsa(gamma=0.99)\nalgorithm.setup(task, policy, value_func)\nalgorithm.run_gpi(test_length)", 
            "title": "Sarsa"
        }, 
        {
            "location": "/algorithm/sarsa/#sarsa-on-policy-td-learning-method", 
            "text": "Sarsa method updates value of state-action pair  Q(s,a) in following way.  s  : current state\na  : action to take at state s choosed by policy PI\nr  : reward by transition (s, a)\ns' : next state after took action a at s\na' : action to take at state s' choosed by policy PI\n\nQ(s,a) = Q(s,a) + alpha [ r + gamma * Q(s', a') - Q(s, a) ]  Update is done with variable  s,a,r,s',a' . So this algorithm is named as Sarsa.  This method is also called as  on-policy TD learning  method.  The keyword is  on-policy .  on-policy  means that Sarsa uses same policy PI to calculate  a  and  a' . \nThe algorithm uses different policy to select  a  and  a'  is called  off-policy  method. (ex. QLearning)", 
            "title": "Sarsa - on-policy TD learning method"
        }, 
        {
            "location": "/algorithm/sarsa/#algorithm", 
            "text": "Parameter:\n    a   - alpha. learning rate. [0,1].\n    g   - gamma. discounting factor. [0,1].\nInitialize:\n    T   - your RL task\n    PI  - policy used in the algorithm\n    Q   - action value function\n\n    Repeat until computational budget runs out:\n        S  - generate initial state of task T\n        A  - choose action at S by following policy PI\n        Repeat until S is terminal state:\n            S'  - next state of S after taking action A\n            R  - reward gained by taking action A at state S\n            A'  - next action at S' by following policy PI\n            Q(S, A)  - Q(S, A) + a * [ R + g * Q(S', A') - Q(S, A)]\n            S, A  - S', A'", 
            "title": "Algorithm"
        }, 
        {
            "location": "/algorithm/sarsa/#value-function", 
            "text": "Sarsa method provides  tabular  and  approximation  type of value functions.", 
            "title": "Value function"
        }, 
        {
            "location": "/algorithm/sarsa/#sarsatabularactionvaluefunction", 
            "text": "If your task is  tabular size , you can use  SarsaTabularActionValueFunction .   If you can store the value of all state-action pair on the memory(array), your task is  tabular  size.   SarsaTabularActionValueFunction  has 3 abstracted method to define the table size of your task.   generate_initial_table  : initialize table object and return it here  fetch_value_from_table  : define how to fetch value from your table  insert_value_into_table  : define how to insert new value into your table   If the shape of your state-action space is SxA, implementation would be like this.  class MyTabularActionValueFunction(SarsaTabularActionValueFunction):\n\n    def generate_initial_table(self):\n        return [[0 for j in range(A)] for i in range(S)]\n\n    def fetch_value_from_table(self, table, state, action):\n        return table[state][action]\n\n    def insert_value_into_table(self, table, state, action, new_value):\n        table[state][action] = new_value", 
            "title": "SarsaTabularActionValueFunction"
        }, 
        {
            "location": "/algorithm/sarsa/#sarsaapproxactionvaluefunction", 
            "text": "If your task is not  tabular  size, you use  SarsaApproxActionValueFunction .  SarsaApproxActionValueFunction  has 3 abstracted methods. You would wrap some prediction model (ex. neuralnet) in these methods.   construct_features  : transform state-action pair into feature representation  approx_predict_value  : predict value of state-action pair with prediction model you want to use  approx_backup  : update your model in supervised learning way with passed input and output pair   The implementation with some neuralnet library would be like this.  class MyApproxActionValueFunction(SarsaApproxActionValueFunction):\n\n    def setup(self):\n        super(MazeApproxActionValueFunction, self).setup()\n        self.neuralnet = build_neuralnet_in_some_way()\n\n    def construct_features(self, state, action):\n        feature1 = do_something(state, action)\n        feature2 = do_anotherthing(state, action)\n        return [feature1, feature2]\n\n    def approx_predict_value(self, features):\n        return self.neuralnet.predict(features)\n\n    def approx_backup(self, features, backup_target, alpha):\n        self.neuralnet.incremental_training(X=features, Y=backup_target)", 
            "title": "SarsaApproxActionValueFunction"
        }, 
        {
            "location": "/algorithm/sarsa/#sample-code-to-start-learning", 
            "text": "test_length = 1000\ntask = MyTask()\npolicy = EpsilonGreedyPolicy(eps=0.1)\nvalue_func = MyTabularActionValueFunction()\nalgorithm = Sarsa(gamma=0.99)\nalgorithm.setup(task, policy, value_func)\nalgorithm.run_gpi(test_length)", 
            "title": "Sample code to start learning"
        }, 
        {
            "location": "/algorithm/q_learning/", 
            "text": "QLearning - off-policy TD learning method\n\n\nQlearning method updates value of state-action pair \nQ(s,a)\nin following way.\n\n\ns  : current state\na  : action to take at state s choosed by policy PI\nr  : reward by transition (s, a)\ns' : next state after took action a at s\nga : greedy action at s' under current value function\n\nQ(s,a) = Q(s,a) + alpha [ r + gamma * Q(s', ga) - Q(s, a) ]\n\n\n\n\nThe new keyword \ngreedy action\n represents the \naction which has maximum estimated value\n under current value function.\n(If multiple actions are greedy then choose one at random.)\n\nYou can get greedy action like this\n\n\nacts = task.generate_possible_actions(state)\nvals = [value_function.predict_value(state, action) for action in acts]\ngreedy_value_and_actions = [(v,a) for v,a in zip(vals, acts) if v==max(vals)]\n_, greedy_action = random.choice(greedy_value_and_actions)\n\n\n\n\nThis method is also called as \noff-policy TD learning\n method.\n\n\noff-policy\n means that this algorithm use different policy to choose \na\n and \na'\n.\n\nQLearning must use \ngreedy policy\n (the policy always choose greedy action) to choose action \na'\n.\n\nBut for choosing \na\n, you can use any policy. (Most of the case this is \nepsilon greedy policy\n)\n\n\nAlgorithm\n\n\nParameter:\n    a  \n- alpha. learning rate. [0,1].\n    g  \n- gamma. discounting factor. [0,1].\nInitialize:\n    T  \n- your RL task\n    PI \n- policy used in the algorithm\n    Q  \n- action value function\n\n    Repeat until computational budget runs out:\n        S \n- generate initial state of task T\n        A \n- choose action at S by following policy PI\n        Repeat until S is terminal state:\n            S' \n- next state of S after taking action A\n            R \n- reward gained by taking action A at state S\n            A' \n- next action at S' by following policy PI\n            GA \n- greedy action at S' under action value function Q\n            Q(S, A) \n- Q(S, A) + a * [ R + g * Q(S', GA) - Q(S, A)]\n            S, A \n- S', A'\n\n\n\n\nValue function\n\n\nQLearning method provides \ntabular\n and \napproximation\n type of value functions.\n\n\nQLearningTabularActionValueFunction\n\n\nIf your task is \ntabular size\n, you can use \nQLearningTabularActionValueFunction\n.\n\n\n\n\nIf you can store the value of all state-action pair on the memory(array), your task is \ntabular\n size.\n\n\n\n\nQLearningTabularActionValueFunction\n has 3 abstracted method to define the table size of your task.\n\n\n\n\ngenerate_initial_table\n : initialize table object and return it here\n\n\nfetch_value_from_table\n : define how to fetch value from your table\n\n\ninsert_value_into_table\n : define how to insert new value into your table\n\n\n\n\nIf the shape of your state-action space is SxA, implementation would be like this.\n\n\nclass MyTabularActionValueFunction(QLearningTabularActionValueFunction):\n\n    def generate_initial_table(self):\n        return [[0 for j in range(A)] for i in range(S)]\n\n    def fetch_value_from_table(self, table, state, action):\n        return table[state][action]\n\n    def insert_value_into_table(self, table, state, action, new_value):\n        table[state][action] = new_value\n\n\n\n\nQLearningApproxActionValueFunction\n\n\nIf your task is not \ntabular\n size, you use \nQLearningApproxActionValueFunction\n.\n\n\nQLearningApproxActionValueFunction\n has 3 abstracted methods. You would wrap some prediction model (ex. neuralnet) in these methods.\n\n\n\n\nconstruct_features\n : transform state-action pair into feature representation\n\n\napprox_predict_value\n : predict value of state-action pair with prediction model you want to use\n\n\napprox_backup\n : update your model in supervised learning way with passed input and output pair\n\n\n\n\nThe implementation with some neuralnet library would be like this.\n\n\nclass MyApproxActionValueFunction(QLearningApproxActionValueFunction):\n\n    def setup(self):\n        super(MazeApproxActionValueFunction, self).setup()\n        self.neuralnet = build_neuralnet_in_some_way()\n\n    def construct_features(self, state, action):\n        feature1 = do_something(state, action)\n        feature2 = do_anotherthing(state, action)\n        return [feature1, feature2]\n\n    def approx_predict_value(self, features):\n        return self.neuralnet.predict(features)\n\n    def approx_backup(self, features, backup_target, alpha):\n        self.neuralnet.incremental_training(X=features, Y=backup_target)\n\n\n\n\nSample code to start learning\n\n\ntest_length = 1000\ntask = MyTask()\npolicy = EpsilonGreedyPolicy(eps=0.1)\nvalue_func = MyTabularActionValueFunction()\nalgorithm = QLearning(gamma=0.99)\nalgorithm.setup(task, policy, value_func)\nalgorithm.run_gpi(test_length)", 
            "title": "QLearning"
        }, 
        {
            "location": "/algorithm/q_learning/#qlearning-off-policy-td-learning-method", 
            "text": "Qlearning method updates value of state-action pair  Q(s,a) in following way.  s  : current state\na  : action to take at state s choosed by policy PI\nr  : reward by transition (s, a)\ns' : next state after took action a at s\nga : greedy action at s' under current value function\n\nQ(s,a) = Q(s,a) + alpha [ r + gamma * Q(s', ga) - Q(s, a) ]  The new keyword  greedy action  represents the  action which has maximum estimated value  under current value function.\n(If multiple actions are greedy then choose one at random.) \nYou can get greedy action like this  acts = task.generate_possible_actions(state)\nvals = [value_function.predict_value(state, action) for action in acts]\ngreedy_value_and_actions = [(v,a) for v,a in zip(vals, acts) if v==max(vals)]\n_, greedy_action = random.choice(greedy_value_and_actions)  This method is also called as  off-policy TD learning  method.  off-policy  means that this algorithm use different policy to choose  a  and  a' . \nQLearning must use  greedy policy  (the policy always choose greedy action) to choose action  a' . \nBut for choosing  a , you can use any policy. (Most of the case this is  epsilon greedy policy )", 
            "title": "QLearning - off-policy TD learning method"
        }, 
        {
            "location": "/algorithm/q_learning/#algorithm", 
            "text": "Parameter:\n    a   - alpha. learning rate. [0,1].\n    g   - gamma. discounting factor. [0,1].\nInitialize:\n    T   - your RL task\n    PI  - policy used in the algorithm\n    Q   - action value function\n\n    Repeat until computational budget runs out:\n        S  - generate initial state of task T\n        A  - choose action at S by following policy PI\n        Repeat until S is terminal state:\n            S'  - next state of S after taking action A\n            R  - reward gained by taking action A at state S\n            A'  - next action at S' by following policy PI\n            GA  - greedy action at S' under action value function Q\n            Q(S, A)  - Q(S, A) + a * [ R + g * Q(S', GA) - Q(S, A)]\n            S, A  - S', A'", 
            "title": "Algorithm"
        }, 
        {
            "location": "/algorithm/q_learning/#value-function", 
            "text": "QLearning method provides  tabular  and  approximation  type of value functions.", 
            "title": "Value function"
        }, 
        {
            "location": "/algorithm/q_learning/#qlearningtabularactionvaluefunction", 
            "text": "If your task is  tabular size , you can use  QLearningTabularActionValueFunction .   If you can store the value of all state-action pair on the memory(array), your task is  tabular  size.   QLearningTabularActionValueFunction  has 3 abstracted method to define the table size of your task.   generate_initial_table  : initialize table object and return it here  fetch_value_from_table  : define how to fetch value from your table  insert_value_into_table  : define how to insert new value into your table   If the shape of your state-action space is SxA, implementation would be like this.  class MyTabularActionValueFunction(QLearningTabularActionValueFunction):\n\n    def generate_initial_table(self):\n        return [[0 for j in range(A)] for i in range(S)]\n\n    def fetch_value_from_table(self, table, state, action):\n        return table[state][action]\n\n    def insert_value_into_table(self, table, state, action, new_value):\n        table[state][action] = new_value", 
            "title": "QLearningTabularActionValueFunction"
        }, 
        {
            "location": "/algorithm/q_learning/#qlearningapproxactionvaluefunction", 
            "text": "If your task is not  tabular  size, you use  QLearningApproxActionValueFunction .  QLearningApproxActionValueFunction  has 3 abstracted methods. You would wrap some prediction model (ex. neuralnet) in these methods.   construct_features  : transform state-action pair into feature representation  approx_predict_value  : predict value of state-action pair with prediction model you want to use  approx_backup  : update your model in supervised learning way with passed input and output pair   The implementation with some neuralnet library would be like this.  class MyApproxActionValueFunction(QLearningApproxActionValueFunction):\n\n    def setup(self):\n        super(MazeApproxActionValueFunction, self).setup()\n        self.neuralnet = build_neuralnet_in_some_way()\n\n    def construct_features(self, state, action):\n        feature1 = do_something(state, action)\n        feature2 = do_anotherthing(state, action)\n        return [feature1, feature2]\n\n    def approx_predict_value(self, features):\n        return self.neuralnet.predict(features)\n\n    def approx_backup(self, features, backup_target, alpha):\n        self.neuralnet.incremental_training(X=features, Y=backup_target)", 
            "title": "QLearningApproxActionValueFunction"
        }, 
        {
            "location": "/algorithm/q_learning/#sample-code-to-start-learning", 
            "text": "test_length = 1000\ntask = MyTask()\npolicy = EpsilonGreedyPolicy(eps=0.1)\nvalue_func = MyTabularActionValueFunction()\nalgorithm = QLearning(gamma=0.99)\nalgorithm.setup(task, policy, value_func)\nalgorithm.run_gpi(test_length)", 
            "title": "Sample code to start learning"
        }, 
        {
            "location": "/algorithm/deep_q_learning/", 
            "text": "deep Q-learning with experience replay.\n\n\nVariant of Q-learning for function approximation proposed in the paper\n\n\nHuman-level control through deep reinforcement learning\n.\n\n\nIn reinforcement leaning, it's known that function approximation with non-linear model (ex. neuralnet) would be unstable and lead poor learning result.\n\nTo address this problem, \ndeep Q-learning\n combined two key ideas with QLearning.\n\n\n\n\nUse \nexperience replay\n to reduce correlations between sequence of learning data.\n\n\nSeparate taget and behavior network\n to stable the source of learning data.\n\n\n\n\nAlgorithm\n\n\n    Parameter\n        g \n- gamma. discounting factor of QLearning\n        N \n- capacity of replay memory\n        C \n- interval to sync Q'(target network) with Q\n        minibatch_size \n- size of minibatch used to train Q\n        replay_start_size \n- initial size of replay memory. Fill D\n            with this number of experiences which created by random policy.\n            This procedure is done in setup phase.\n\n    Initialize\n        T  \n- your RL task\n        PI \n- policy used to select action during the episode\n        Q  \n- approximate action value function (ex. neural network)\n        Q' \n- target network. initialized by deepcopy Q.\n        D  \n- filled with replay_start_size of  experiences created by random simulation.\n              (experience = tuple of state, action, reward and next_state)\n\n    Repeat until computational budget runs out:\n        S \n- generate initial state of task T\n        A \n- choose action at S by following PI\n        Repeat until S is terminal state:\n            S' \n- next state of S after taking action A\n            R \n- reward gained by taking action A at state S\n            A' \n- next action at S' by following policy PI\n            append experience (S,A,R,S') to D\n            MB \n- sample minibatch_size of experiences from D\n            BT \n- transform minibatch of experiences into backup targets\n            (BT = [r + g * Q(s', GA) for s,a,r,s' in MB], GA=greedy action at s')\n            Update Q by using BT (minibatch of backup targets)\n            Every C step: Q' \n- Q (ex. deepcopy weights of Q to Q')\n            S, A \n- S', A'\n\n\n\n\nValue function\n\n\nDeepQLearning\n method provides only \napproximation\n type of value functions.\n\n\nDeepQLearningApproxActionValueFunction\n\n\nDeepQLearningApproxActionValueFunction\n has 6 abstracted methods. You would wrap your prediction model (ex. neuralnet) in these methods.\n\n\n\n\ninitialize_network\n : initialize your prediction model here\n\n\ndeepcopy_network\n : define how to create deepcopy of your prediction model\n\n\npredict_value_by_network\n : predict value of state-action pair by your prediction model\n\n\nbackup_on_minibatch\n : train your prediction model with passed learning minibatch\n\n\nsave_networks\n : save your prediction model as you like (ex. save the weights of neuralnet)\n\n\nload_networks\n : load your prediction model from resource created by \nsave_networks\n\n\n\n\nThe implementation with some neuralnet library would be like this.\n\n\nclass MyApproxActionValueFunction(DeepQLearningApproxActionValueFunction):\n\n    # the model returned here is used as \nq_network\n (Q of above algorithm)\n    def initialize_network(self):\n        model = build_neuralnet()\n        return model\n\n    # the model returned here is used as \nq_hat_network\n (Q' of above algorithm)\n    def deepcopy_network(self, q_network):\n        original_weight = q_network.get_weights()\n        deepcopy_network = self.initialize_network()\n        deepcopy_network.set_weights(original_weight)\n        return deepcopy_network\n\n    # return prediction value of passed state action pair.\n    # passed network would be \nq_network\n or \nq_hat_network\n.\n    def predict_value_by_network(self, network, state, action):\n        features = build_features(state, action)\n        prediction = network.predict(features)\n        return prediction\n\n    # train passed q_network with backup_minibatch\n    # you would need to transform backup_minibatch into input output pair like\n    # supervised learning format.\n    def backup_on_minibatch(self, q_network, backup_minibatch):\n        # backup_minibatch is array of (state, action, target_value).\n        X = [build_features(state, action) for state, action, _target in backup_minibatch]\n        y = [target for _state, _action, target in backup_minibatch]\n        q_network.train_on_minibatch(X, y)\n\n    # save passed two neuralnet on passed directory\n    def save_networks(self, q_network, q_hat_network, save_dir_path):\n        q_network.save_weights(\n%s/q_weight.h5\n % save_dir_path)\n        q_hat_network.save_weights(\n%s/q_hat_weight.h5\n % save_dir_path)\n\n    # load \nq_network\n and \nq_hat_network\n from passed directory and return them in\n    # \nq_network\n, \nq_hat_network\n order.\n    def load_networks(self, load_dir_path):\n        q_network = self.initialize_network()\n        q_network.load_weights(\n%s/q_weight.h5\n % load_dir_path)\n        q_hat_network = self.initialize_network()\n        q_hat_network.load_weights(\n%s/q_hat_weight.h5\n % load_dir_path)\n        return q_network, q_hat_network\n\n\n\n\n\nSample code to start learning\n\n\nTEST_LENGTH = 5000000\ntask = MyTask()\npolicy = EpsilonGreedyPolicy(eps=1.0)\npolicy.set_eps_annealing(initial_eps=1.0, final_eps=0.1, anneal_duration=1000000)\nvalue_func = MyApproxActionValueFunction()\nalgorithm = DeepQLearning(gamma=0.99, N=100000, C=1000, minibatch_size=32, replay_start_size=50000)\nalgorithm.setup(task, policy, value_func)\nalgorithm.run_gpi(test_length)", 
            "title": "deep Q-learning"
        }, 
        {
            "location": "/algorithm/deep_q_learning/#deep-q-learning-with-experience-replay", 
            "text": "Variant of Q-learning for function approximation proposed in the paper  Human-level control through deep reinforcement learning .  In reinforcement leaning, it's known that function approximation with non-linear model (ex. neuralnet) would be unstable and lead poor learning result. \nTo address this problem,  deep Q-learning  combined two key ideas with QLearning.   Use  experience replay  to reduce correlations between sequence of learning data.  Separate taget and behavior network  to stable the source of learning data.", 
            "title": "deep Q-learning with experience replay."
        }, 
        {
            "location": "/algorithm/deep_q_learning/#algorithm", 
            "text": "Parameter\n        g  - gamma. discounting factor of QLearning\n        N  - capacity of replay memory\n        C  - interval to sync Q'(target network) with Q\n        minibatch_size  - size of minibatch used to train Q\n        replay_start_size  - initial size of replay memory. Fill D\n            with this number of experiences which created by random policy.\n            This procedure is done in setup phase.\n\n    Initialize\n        T   - your RL task\n        PI  - policy used to select action during the episode\n        Q   - approximate action value function (ex. neural network)\n        Q'  - target network. initialized by deepcopy Q.\n        D   - filled with replay_start_size of  experiences created by random simulation.\n              (experience = tuple of state, action, reward and next_state)\n\n    Repeat until computational budget runs out:\n        S  - generate initial state of task T\n        A  - choose action at S by following PI\n        Repeat until S is terminal state:\n            S'  - next state of S after taking action A\n            R  - reward gained by taking action A at state S\n            A'  - next action at S' by following policy PI\n            append experience (S,A,R,S') to D\n            MB  - sample minibatch_size of experiences from D\n            BT  - transform minibatch of experiences into backup targets\n            (BT = [r + g * Q(s', GA) for s,a,r,s' in MB], GA=greedy action at s')\n            Update Q by using BT (minibatch of backup targets)\n            Every C step: Q'  - Q (ex. deepcopy weights of Q to Q')\n            S, A  - S', A'", 
            "title": "Algorithm"
        }, 
        {
            "location": "/algorithm/deep_q_learning/#value-function", 
            "text": "DeepQLearning  method provides only  approximation  type of value functions.", 
            "title": "Value function"
        }, 
        {
            "location": "/algorithm/deep_q_learning/#deepqlearningapproxactionvaluefunction", 
            "text": "DeepQLearningApproxActionValueFunction  has 6 abstracted methods. You would wrap your prediction model (ex. neuralnet) in these methods.   initialize_network  : initialize your prediction model here  deepcopy_network  : define how to create deepcopy of your prediction model  predict_value_by_network  : predict value of state-action pair by your prediction model  backup_on_minibatch  : train your prediction model with passed learning minibatch  save_networks  : save your prediction model as you like (ex. save the weights of neuralnet)  load_networks  : load your prediction model from resource created by  save_networks   The implementation with some neuralnet library would be like this.  class MyApproxActionValueFunction(DeepQLearningApproxActionValueFunction):\n\n    # the model returned here is used as  q_network  (Q of above algorithm)\n    def initialize_network(self):\n        model = build_neuralnet()\n        return model\n\n    # the model returned here is used as  q_hat_network  (Q' of above algorithm)\n    def deepcopy_network(self, q_network):\n        original_weight = q_network.get_weights()\n        deepcopy_network = self.initialize_network()\n        deepcopy_network.set_weights(original_weight)\n        return deepcopy_network\n\n    # return prediction value of passed state action pair.\n    # passed network would be  q_network  or  q_hat_network .\n    def predict_value_by_network(self, network, state, action):\n        features = build_features(state, action)\n        prediction = network.predict(features)\n        return prediction\n\n    # train passed q_network with backup_minibatch\n    # you would need to transform backup_minibatch into input output pair like\n    # supervised learning format.\n    def backup_on_minibatch(self, q_network, backup_minibatch):\n        # backup_minibatch is array of (state, action, target_value).\n        X = [build_features(state, action) for state, action, _target in backup_minibatch]\n        y = [target for _state, _action, target in backup_minibatch]\n        q_network.train_on_minibatch(X, y)\n\n    # save passed two neuralnet on passed directory\n    def save_networks(self, q_network, q_hat_network, save_dir_path):\n        q_network.save_weights( %s/q_weight.h5  % save_dir_path)\n        q_hat_network.save_weights( %s/q_hat_weight.h5  % save_dir_path)\n\n    # load  q_network  and  q_hat_network  from passed directory and return them in\n    #  q_network ,  q_hat_network  order.\n    def load_networks(self, load_dir_path):\n        q_network = self.initialize_network()\n        q_network.load_weights( %s/q_weight.h5  % load_dir_path)\n        q_hat_network = self.initialize_network()\n        q_hat_network.load_weights( %s/q_hat_weight.h5  % load_dir_path)\n        return q_network, q_hat_network", 
            "title": "DeepQLearningApproxActionValueFunction"
        }, 
        {
            "location": "/algorithm/deep_q_learning/#sample-code-to-start-learning", 
            "text": "TEST_LENGTH = 5000000\ntask = MyTask()\npolicy = EpsilonGreedyPolicy(eps=1.0)\npolicy.set_eps_annealing(initial_eps=1.0, final_eps=0.1, anneal_duration=1000000)\nvalue_func = MyApproxActionValueFunction()\nalgorithm = DeepQLearning(gamma=0.99, N=100000, C=1000, minibatch_size=32, replay_start_size=50000)\nalgorithm.setup(task, policy, value_func)\nalgorithm.run_gpi(test_length)", 
            "title": "Sample code to start learning"
        }, 
        {
            "location": "/algorithm/monte_carlo_tree_search/", 
            "text": "MonteCarloTreeSearch\n\n\nMonte Carlo Tree Search (MCTS) is one of the famous \nheuristic search\n method. MCTS builds search tree to find most promising action by using results of random simulation.  \n\n\nUnlike other reinforcement algorithms (ex. MonteCarlo, QLearning, ...), MCTS has no training phase.\n\nMCTS finds most promising action every time when it receives new state to choose action.  \n\n\nFor more detail explanation see \nA Survey of Monte Carlo Tree Search Methods\n.\n\n(We implemented MCTS based on this paper.)\n\n\nAlgorithm\n\n\nThe search tree of MCTS represents search space of reinforcement learning task.\n\nEach node represents some state \nS\n and its child edge represents possible actions at state \nS\n.  \n\n\nFirst we pass current state of task to MCTS. Then MCTS creates search tree which has only root node representing current state.\nAfter that MCTS starts to build search tree by iterating following 4 steps as long as possible.\n\n\n\n\nSELECT : select a node which has not expanded edge\n\n\nEXPAND : add child node of selected node on search tree\n\n\nPLAYOUT : run random simulation from state which expanded node representing\n\n\nBACKPROPAGATION : backpropagate reward of simulation from expanded node to root node\n\n\n\n\nInitialize:\n    R \n- Build game tree which has only a root node which represents\n         current state (where we want to find best action)\nRepeat until computational budget runs out:\n    N  \n- find not expanded node by descending R from root node\n         (expanded = visited all child node at least once)\n    N' \n- child node of N which is not visited yet.\n          And add N' on search tree.\n    R  \n- reward of simulation started from the state which N' represents\n    Backpropagate R from N' to R\nreturn best action(edge of highest value) at R\n\n\n\n\nTutorial\n\n\nNow we will introduce how to use MCTS with example task \nTickTackToeTask\n.  \n\n\nBefore starting implementation, we need to decide the algorithm for \nSELECT\n step (how to choose the node to expand).\nIn this tutorial we adapt famous algorithm \nUCT search\n (Most of the case this choice would be good).\n\n\nUCT search\n calculate the value of edge by following equation.  \n\n\nedge_value = average_reard + 2 * C * sqrt( 2 * log(N) / n )\n\n\n\n\nwhere\n\n\n\n\naverage_reward\n is the average of reward received through \nBACKPROPAGATION\n step.\n\n\nC\n is the hyper parameter to balance explore and exploitation\n\n\nN\n total visit count of parent node of target edge\n\n\nn\n total visit count of child node of target edge\n\n\n\n\nAnd descend to the edge of child node which has maximum value.  \n\n\nThis algorithm is already implemented as \nUCTNode\n, \nUCTEdge\n classes. So we will use it.\n\n\nCreating custom node\n\n\ntick-tack-toe\n is two-player zero-sum game. So the best action of opponent player is the worst action of my player.\n\nTo integrate this idea with UCT search, we create \nMaxNode\n and \nMinNode\n.\n\n\nfrom kyoka.algorithm.montecarlo_tree_search import UCTNode\n\n# descend the tree to the child edge which has maximum value. (choose best action for me)\n# This node should represents the state of my turn.\nclass MaxNode(BaseNode):\n\n    def select_best_edge(self):\n        sort_key = lambda edge: edge.calculate_value()\n        max_val_edge = sorted([edge for edge in self.child_edges], key=sort_key)[-1]\n        return max_val_edge\n\n# descend the tree to the child edge which has minimum value. (choose worst action for me)\n# This node should represents the state of opponent turn.\nclass MinNode(BaseNode):\n\n    def select_best_edge(self):\n        sort_key = lambda edge: edge.calculate_value()\n        min_val_edge = sorted([edge for edge in self.child_edges], key=sort_key)[0]\n        return min_val_edge\n\n\n\n\nAfter finished to create \nNode\n class, we need to tell MCTS how to build our search tree.\n\nWe will do this by implementing \nBaseMCTS.generate_node_from_state\n method.\n\n\nfrom kyoka.algorithm.montecarlo_tree_search import BaseMCTS\n\nclass MyMCTS(BaseMCTS):\n\n    def generate_node_from_state(self, state):\n        if self.next_player_is_me(state):\n            return MaxNode(state)\n        else:\n            return MinNode(state)\n\n    def next_player_is_me(self, state):\n        return # TODO judge passed state is my turn or not by some logic\n\n\n\n\nOk, we prepared everything.\n\nBelow code runs 5000 iteration of MCTS and returns most promising action at initial state.\n\n\nfrom kyoka.callback import WatchIterationCount\n\nNB_SIMULATION = 5000\nfinish_rule = WatchIterationCount(SIMULATION_NUM)\ntask = TickTackToeTask()\nalgorithm = MyMCTS(TickTackToeTask)\n\nstate = task.generate_initial_state()\nbest_action = algorithm.planning(state, finish_rule)\n\n# you can also call planning method through \nchoose_action(task, value_functoin, state)\n interface\nalgorithm.set_finish_rule(finish_rule)\nbest_action = algorithm.choose_action(\ndummy\n, \ndummy\n, state)\n\n\n\n\nAdvanced\n\n\nIf you want to run simulation in not random way, you can customize simulation logic.\n\nSimulation is executed by calling method from \nBaseMCTS.playout_policy\n property.\n\nSo you can create simulation method and set it by calling \nBaseMCTS.set_playout_policy\n method.  \n\n\nThe interface of simulation method is\n\n\n\n\nreceives \ntask\n and \nleaf_node\n as argument\n\n\nreturns reward of simulation\n\n\n\n\nThe default implementation \nrandom_playout\n is implemented like this.\n\n\nimport random\n\ndef random_playout(task, leaf_node):\n    state = leaf_node.state\n    while not task.is_terminal_state(state):\n        actions = task.generate_possible_actions(state)\n        action = random.choice(actions)\n        state = task.transit_state(state, action)\n    return task.calculate_reward(state)\n\nmcts.set_playout_policy(random_playout)", 
            "title": "MonteCarloTreeSearch"
        }, 
        {
            "location": "/algorithm/monte_carlo_tree_search/#montecarlotreesearch", 
            "text": "Monte Carlo Tree Search (MCTS) is one of the famous  heuristic search  method. MCTS builds search tree to find most promising action by using results of random simulation.    Unlike other reinforcement algorithms (ex. MonteCarlo, QLearning, ...), MCTS has no training phase. \nMCTS finds most promising action every time when it receives new state to choose action.    For more detail explanation see  A Survey of Monte Carlo Tree Search Methods . \n(We implemented MCTS based on this paper.)", 
            "title": "MonteCarloTreeSearch"
        }, 
        {
            "location": "/algorithm/monte_carlo_tree_search/#algorithm", 
            "text": "The search tree of MCTS represents search space of reinforcement learning task. \nEach node represents some state  S  and its child edge represents possible actions at state  S .    First we pass current state of task to MCTS. Then MCTS creates search tree which has only root node representing current state.\nAfter that MCTS starts to build search tree by iterating following 4 steps as long as possible.   SELECT : select a node which has not expanded edge  EXPAND : add child node of selected node on search tree  PLAYOUT : run random simulation from state which expanded node representing  BACKPROPAGATION : backpropagate reward of simulation from expanded node to root node   Initialize:\n    R  - Build game tree which has only a root node which represents\n         current state (where we want to find best action)\nRepeat until computational budget runs out:\n    N   - find not expanded node by descending R from root node\n         (expanded = visited all child node at least once)\n    N'  - child node of N which is not visited yet.\n          And add N' on search tree.\n    R   - reward of simulation started from the state which N' represents\n    Backpropagate R from N' to R\nreturn best action(edge of highest value) at R", 
            "title": "Algorithm"
        }, 
        {
            "location": "/algorithm/monte_carlo_tree_search/#tutorial", 
            "text": "Now we will introduce how to use MCTS with example task  TickTackToeTask .    Before starting implementation, we need to decide the algorithm for  SELECT  step (how to choose the node to expand).\nIn this tutorial we adapt famous algorithm  UCT search  (Most of the case this choice would be good).  UCT search  calculate the value of edge by following equation.    edge_value = average_reard + 2 * C * sqrt( 2 * log(N) / n )  where   average_reward  is the average of reward received through  BACKPROPAGATION  step.  C  is the hyper parameter to balance explore and exploitation  N  total visit count of parent node of target edge  n  total visit count of child node of target edge   And descend to the edge of child node which has maximum value.    This algorithm is already implemented as  UCTNode ,  UCTEdge  classes. So we will use it.", 
            "title": "Tutorial"
        }, 
        {
            "location": "/algorithm/monte_carlo_tree_search/#creating-custom-node", 
            "text": "tick-tack-toe  is two-player zero-sum game. So the best action of opponent player is the worst action of my player. \nTo integrate this idea with UCT search, we create  MaxNode  and  MinNode .  from kyoka.algorithm.montecarlo_tree_search import UCTNode\n\n# descend the tree to the child edge which has maximum value. (choose best action for me)\n# This node should represents the state of my turn.\nclass MaxNode(BaseNode):\n\n    def select_best_edge(self):\n        sort_key = lambda edge: edge.calculate_value()\n        max_val_edge = sorted([edge for edge in self.child_edges], key=sort_key)[-1]\n        return max_val_edge\n\n# descend the tree to the child edge which has minimum value. (choose worst action for me)\n# This node should represents the state of opponent turn.\nclass MinNode(BaseNode):\n\n    def select_best_edge(self):\n        sort_key = lambda edge: edge.calculate_value()\n        min_val_edge = sorted([edge for edge in self.child_edges], key=sort_key)[0]\n        return min_val_edge  After finished to create  Node  class, we need to tell MCTS how to build our search tree. \nWe will do this by implementing  BaseMCTS.generate_node_from_state  method.  from kyoka.algorithm.montecarlo_tree_search import BaseMCTS\n\nclass MyMCTS(BaseMCTS):\n\n    def generate_node_from_state(self, state):\n        if self.next_player_is_me(state):\n            return MaxNode(state)\n        else:\n            return MinNode(state)\n\n    def next_player_is_me(self, state):\n        return # TODO judge passed state is my turn or not by some logic  Ok, we prepared everything. \nBelow code runs 5000 iteration of MCTS and returns most promising action at initial state.  from kyoka.callback import WatchIterationCount\n\nNB_SIMULATION = 5000\nfinish_rule = WatchIterationCount(SIMULATION_NUM)\ntask = TickTackToeTask()\nalgorithm = MyMCTS(TickTackToeTask)\n\nstate = task.generate_initial_state()\nbest_action = algorithm.planning(state, finish_rule)\n\n# you can also call planning method through  choose_action(task, value_functoin, state)  interface\nalgorithm.set_finish_rule(finish_rule)\nbest_action = algorithm.choose_action( dummy ,  dummy , state)", 
            "title": "Creating custom node"
        }, 
        {
            "location": "/algorithm/monte_carlo_tree_search/#advanced", 
            "text": "If you want to run simulation in not random way, you can customize simulation logic. \nSimulation is executed by calling method from  BaseMCTS.playout_policy  property. \nSo you can create simulation method and set it by calling  BaseMCTS.set_playout_policy  method.    The interface of simulation method is   receives  task  and  leaf_node  as argument  returns reward of simulation   The default implementation  random_playout  is implemented like this.  import random\n\ndef random_playout(task, leaf_node):\n    state = leaf_node.state\n    while not task.is_terminal_state(state):\n        actions = task.generate_possible_actions(state)\n        action = random.choice(actions)\n        state = task.transit_state(state, action)\n    return task.calculate_reward(state)\n\nmcts.set_playout_policy(random_playout)", 
            "title": "Advanced"
        }, 
        {
            "location": "/callback/about_callback/", 
            "text": "What is callback\n\n\nYou can define some procedure which you want to execute in the middle of training through callback object.\n\n(ex. record performance of value function in each 1000 iteration of training.)  \n\n\nHow to set callback for training is createing list of callbacks and pass it to \nalgorithm.run_gpi(nb_iteration, callbacks=None)\n.\n\n(You can also pass single callback object (not list) as callbacks arguments)\n\n\nCallback methods\n\n\nThe base class of all callback objects is \nkyoka.callback.BaseCallback\n.\n\nAll callback object must inherit this class and override callback methods as you want.  \n\n\nkyoka.callback.BaseCallback\n class has 5 callback methods which callback object can override.  \n\n\n\n\nbefore_gpi_start(self, task, value_function)\n\n\ncalled when \nalgorithm.run_gpi\n is called\n\n\n\n\n\n\nbefore_update(self, iteration_count, task, value_function)\n\n\ncalled before \niteration_count\n of episode is played in training\n\n\n\n\n\n\nafter_update(self, iteration_count, task, value_function)\n\n\ncalled after \niteration_count\n of episode is played in training\n\n\n\n\n\n\nafter_gpi_finish(self, task, value_function)\n\n\ncalled when training finishes\n\n\n\n\n\n\ninterrupt_gpi(self, iteration_count, task, value_function)\n\n\nif you return \nTrue\n training finishes even if it doesn't reach maximum iteration count\n\n\n\n\n\n\n\n\nIn default, above 4 methods are implemented as empty method and \ninterrupt_gpi\n just returns \nFalse\n.\n\nSo callback object don't need to override all methods if nothing to do.\n\n\nLogging methods\n\n\nkyoka.callback.BaseCallback\n class also have utility method \nlog(message)\n.\n\nIf you want to log something on console, we recommend you to use this method instead of \nprint(message)\n.\n\n\nlog(message)\n prints passed message with tag like below.  \n\n\n callback = WatchIterationCount(5000)\n\n callback.log(\nStart GPI iteration for 5000 times\n)\n[WatchIterationCount] Start GPI iteration for 5000 times\n\n\n\n\nWe use class name of callback as tag in default. But you can easily customize it by overriding \ndefine_log_tag\n method.  \n\n\nclass WatchIterationCount(BaseCallback):\n    # some codes...\n\n    def define_log_tag(self):\n        return \nProgress\n\n\n\n callback = WatchIterationCount(5000)\n\n callback.log(\nStart GPI iteration for 5000 times\n)\n[Progress] Start GPI iteration for 5000 times\n\n\n\n\n\n\nHere is the sample custom callback to record value of initial state in every 1000 iteration.\n\n\nimport csv\nfrom kyoka.policy import choose_best_action\n\nclass InitialStateValueRecorder(BaseCallback):\n\n    def __init__(self, score_file_path):\n        self.score_file_path = score_file_path\n        self.score_holder = []\n\n    def before_gpi_start(self, task, value_function):\n        value = self._predict_value_of_initial_state(task, value_function)\n        self.log(\nValue of initial state is [ %s ]\n % value)\n        self.score_holder.append(value)\n\n    def after_update(self, iteration_count, task, value_function):\n        value = self._predict_value_of_initial_state(task, value_function)\n        self.log(\nValue of initial state is [ %s ]\n % value)\n        self.score_holder.append(value)\n\n    def after_gpi_finish(self, task, value_function):\n        with open(self.score_file_path, \nwb\n) as f:\n            writer = csv.writer(f, lineterminator=\n\\n\n)\n            writer.writerow(self.score_holder)\n        self.log(\nScore is saved on [ %s ]\n % self.score_file_path)\n\n\n    def _predict_value_of_initial_state(self, task, value_function):\n        state = task.generate_initial_state()\n        action = choose_best_action(task, value_function, state)\n        return value_function.predict_value(state, action)", 
            "title": "About Callback"
        }, 
        {
            "location": "/callback/about_callback/#what-is-callback", 
            "text": "You can define some procedure which you want to execute in the middle of training through callback object. \n(ex. record performance of value function in each 1000 iteration of training.)    How to set callback for training is createing list of callbacks and pass it to  algorithm.run_gpi(nb_iteration, callbacks=None) . \n(You can also pass single callback object (not list) as callbacks arguments)", 
            "title": "What is callback"
        }, 
        {
            "location": "/callback/about_callback/#callback-methods", 
            "text": "The base class of all callback objects is  kyoka.callback.BaseCallback . \nAll callback object must inherit this class and override callback methods as you want.    kyoka.callback.BaseCallback  class has 5 callback methods which callback object can override.     before_gpi_start(self, task, value_function)  called when  algorithm.run_gpi  is called    before_update(self, iteration_count, task, value_function)  called before  iteration_count  of episode is played in training    after_update(self, iteration_count, task, value_function)  called after  iteration_count  of episode is played in training    after_gpi_finish(self, task, value_function)  called when training finishes    interrupt_gpi(self, iteration_count, task, value_function)  if you return  True  training finishes even if it doesn't reach maximum iteration count     In default, above 4 methods are implemented as empty method and  interrupt_gpi  just returns  False . \nSo callback object don't need to override all methods if nothing to do.", 
            "title": "Callback methods"
        }, 
        {
            "location": "/callback/about_callback/#logging-methods", 
            "text": "kyoka.callback.BaseCallback  class also have utility method  log(message) . \nIf you want to log something on console, we recommend you to use this method instead of  print(message) .  log(message)  prints passed message with tag like below.     callback = WatchIterationCount(5000)  callback.log( Start GPI iteration for 5000 times )\n[WatchIterationCount] Start GPI iteration for 5000 times  We use class name of callback as tag in default. But you can easily customize it by overriding  define_log_tag  method.    class WatchIterationCount(BaseCallback):\n    # some codes...\n\n    def define_log_tag(self):\n        return  Progress   callback = WatchIterationCount(5000)  callback.log( Start GPI iteration for 5000 times )\n[Progress] Start GPI iteration for 5000 times   Here is the sample custom callback to record value of initial state in every 1000 iteration.  import csv\nfrom kyoka.policy import choose_best_action\n\nclass InitialStateValueRecorder(BaseCallback):\n\n    def __init__(self, score_file_path):\n        self.score_file_path = score_file_path\n        self.score_holder = []\n\n    def before_gpi_start(self, task, value_function):\n        value = self._predict_value_of_initial_state(task, value_function)\n        self.log( Value of initial state is [ %s ]  % value)\n        self.score_holder.append(value)\n\n    def after_update(self, iteration_count, task, value_function):\n        value = self._predict_value_of_initial_state(task, value_function)\n        self.log( Value of initial state is [ %s ]  % value)\n        self.score_holder.append(value)\n\n    def after_gpi_finish(self, task, value_function):\n        with open(self.score_file_path,  wb ) as f:\n            writer = csv.writer(f, lineterminator= \\n )\n            writer.writerow(self.score_holder)\n        self.log( Score is saved on [ %s ]  % self.score_file_path)\n\n\n    def _predict_value_of_initial_state(self, task, value_function):\n        state = task.generate_initial_state()\n        action = choose_best_action(task, value_function, state)\n        return value_function.predict_value(state, action)", 
            "title": "Logging methods"
        }, 
        {
            "location": "/callback/callbacks/", 
            "text": "Callbacks implemented by \nkyoka\n\n\nkyoka\n prepared callbacks which would be useful for reinforcement learning.  \n\n\n\n\nLearningRecorder\n\n\nSave algorithm in the middle of training in each specified interval.\n\n\nLearningRecorder(algorithm, root_save_dir_path, save_interval)\n\n\n\n\nIf you set \nroot_save_dir_path=\"dev/rl/training_results\n, \nsave_interval=1000\n,\n\nafter 2500 iteration of training, the directory of \nroot_save_dir_path\n has two items like below\n\n\n ls dev/rl/training_results\nafter_1000_iteration        after_2000_iteration\n\n\n\n\nIf you want to load training results of ater 1000 iteration, you would ...\n\n\nalgorithm.load(\ndev/rl/training_results/after_1000_iteration\n)\n\n\n\n\n\n\nBasePerformanceWatcher\n\n\nExecute some calculation with task and value function in the middle of training and logs its result.\n\n\nThis class has 2 abstracted methods you need to implement.\n\n\n\n\nrun_performance_test(self, task, value_function)\n: run some calculation and returns its result\n\n\ndefine_performance_test_interval\n : define the interval of training to execute \nrun_performance_test\n\n\n\n\nBelow implementation checks how much rewards gained in the episode by intermediate value function \nand logs it in each 5000 training iteration.  \n\n\nfrom kyoka.callback import BasePerformanceWatcher\nfrom kyoka.algorithm.rl_algorithm import generate_episode\nfrom kyoka.policy import GreedyPolicy\n\nclass RewardsPeformanceWatcher(BasePerformanceWatcher):\n\n    def setUp(self, task, value_function):\n        self.policy = GreedyPolicy()\n\n    def tearDown(self, task, value_function):\n        pass\n\n    def define_performance_test_interval(self):\n        return 5000\n\n    def run_performance_test(self, task, value_function):\n        episode = generate_episode(task, self.policy, value_function)\n        gains = sum([reward for _state, _action, _next_state, reward in episode])\n        return gains\n\n    # This is the default implementation to generate log message.\n    # So if this implementation is ok, you do not need to implement this method.\n    # Argument \ntest_result\n is the item which you returned in \nrun_performance_test\n\n    def define_log_message(self, iteration_count, task, value_function, test_result):\n        base_msg = \nPerformance test result : %s (nb_iteration=%d)\n\n        return base_msg % (test_result, iteration_count)\n\n\n\n\n\n\nManualInterruption\n\n\nYou can stop training whenever you want by writing \"stop\" on specified file.\n\n\nManualInterruption(monitor_file_path, watch_interval=30)\n\n\n\n\nIf you pass \nmonitor_file_path=dev/rl/stop.txt\n then this callback checks\n\n\n\n\nif a file exists on \nmonitor_file_path\n\n\nif a file exists, find words \"stop\" in the file\n\n\nif found the word \"stop\", finish the training\n\n\n\n\nin each 30 iteration of training.  \n\n\nSo you can interrupt training like this.\n\n\necho stop \n dev/rl/stop.txt", 
            "title": "Callbacks"
        }, 
        {
            "location": "/callback/callbacks/#callbacks-implemented-by-kyoka", 
            "text": "kyoka  prepared callbacks which would be useful for reinforcement learning.", 
            "title": "Callbacks implemented by kyoka"
        }, 
        {
            "location": "/callback/callbacks/#learningrecorder", 
            "text": "Save algorithm in the middle of training in each specified interval.  LearningRecorder(algorithm, root_save_dir_path, save_interval)  If you set  root_save_dir_path=\"dev/rl/training_results ,  save_interval=1000 , \nafter 2500 iteration of training, the directory of  root_save_dir_path  has two items like below   ls dev/rl/training_results\nafter_1000_iteration        after_2000_iteration  If you want to load training results of ater 1000 iteration, you would ...  algorithm.load( dev/rl/training_results/after_1000_iteration )", 
            "title": "LearningRecorder"
        }, 
        {
            "location": "/callback/callbacks/#baseperformancewatcher", 
            "text": "Execute some calculation with task and value function in the middle of training and logs its result.  This class has 2 abstracted methods you need to implement.   run_performance_test(self, task, value_function) : run some calculation and returns its result  define_performance_test_interval  : define the interval of training to execute  run_performance_test   Below implementation checks how much rewards gained in the episode by intermediate value function \nand logs it in each 5000 training iteration.    from kyoka.callback import BasePerformanceWatcher\nfrom kyoka.algorithm.rl_algorithm import generate_episode\nfrom kyoka.policy import GreedyPolicy\n\nclass RewardsPeformanceWatcher(BasePerformanceWatcher):\n\n    def setUp(self, task, value_function):\n        self.policy = GreedyPolicy()\n\n    def tearDown(self, task, value_function):\n        pass\n\n    def define_performance_test_interval(self):\n        return 5000\n\n    def run_performance_test(self, task, value_function):\n        episode = generate_episode(task, self.policy, value_function)\n        gains = sum([reward for _state, _action, _next_state, reward in episode])\n        return gains\n\n    # This is the default implementation to generate log message.\n    # So if this implementation is ok, you do not need to implement this method.\n    # Argument  test_result  is the item which you returned in  run_performance_test \n    def define_log_message(self, iteration_count, task, value_function, test_result):\n        base_msg =  Performance test result : %s (nb_iteration=%d) \n        return base_msg % (test_result, iteration_count)", 
            "title": "BasePerformanceWatcher"
        }, 
        {
            "location": "/callback/callbacks/#manualinterruption", 
            "text": "You can stop training whenever you want by writing \"stop\" on specified file.  ManualInterruption(monitor_file_path, watch_interval=30)  If you pass  monitor_file_path=dev/rl/stop.txt  then this callback checks   if a file exists on  monitor_file_path  if a file exists, find words \"stop\" in the file  if found the word \"stop\", finish the training   in each 30 iteration of training.    So you can interrupt training like this.  echo stop   dev/rl/stop.txt", 
            "title": "ManualInterruption"
        }
    ]
}