<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>MonteCarlo - kyoka</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "MonteCarlo";
    var mkdocs_page_input_path = "algorithm/montecarlo.md";
    var mkdocs_page_url = "/algorithm/montecarlo/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> kyoka</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <ul class="subnav">
    <li><span>Home</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../..">Introduction</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Tutorial</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../tutorial/tabular_method_tutorial/">Tabular Method Tutorial</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../tutorial/approximation_method_tutorial/">Approximation Method Tutorial</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Algorithm</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../about_algorithms/">About Algorithms</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">MonteCarlo</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#montecarlo-method">MonteCarlo Method</a></li>
                
                    <li><a class="toctree-l4" href="#algorithm">Algorithm</a></li>
                
                    <li><a class="toctree-l4" href="#reward-discounting">Reward discounting</a></li>
                
                    <li><a class="toctree-l4" href="#value-function">Value function</a></li>
                
            
            </ul>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../sarsa/">Sarsa</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../q_learning/">QLearning</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../deep_q_learning/">deep Q-learning</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../monte_carlo_tree_search/">MonteCarloTreeSearch</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Callback</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../callback/about_callback/">About Callback</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../callback/callbacks/">Callbacks</a>
        
    </li>

        
    </ul>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">kyoka</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Algorithm &raquo;</li>
        
      
    
    <li>MonteCarlo</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/ishikota/kyoka/edit/master/docs/algorithm/montecarlo.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="montecarlo-method">MonteCarlo Method</h1>
<p>MonteCarlo method estimates the value of action by averaging result of random simulation.  </p>
<h2 id="algorithm">Algorithm</h2>
<p>Our implementation of MonteCarlo method is the one called as <em>every-visit MonteCarlo method</em>.<br />
<strong>every-visit</strong> means <em>using every state for update in an episode even if same state appeared in the episode</em>.</p>
<pre><code class="bash">Parameter:
    g  &lt;- gamma. discounting factor for reward. [0,1]
Initialize:
    T  &lt;- your RL task
    PI &lt;- Policy used to generate episode
    Q  &lt;- action value function

Repeat until computational budget runs out:
    generate an episode of T by following policy PI
    for each state-action pair (S, A)  appeared in the episode:
        G &lt;- sum of rewards gained after state S (G is discounted if g &lt; 1)
        Q(S, A) &lt;- average G of S sampled ever
</code></pre>

<h2 id="reward-discounting">Reward discounting</h2>
<p>We support <em>reward discounting</em> feature.<br />
If you want to use this feature, set parameter <code>gamma</code> in range [0, 1).<br />
(<code>gamma</code> is set to 1 as default value. This means no discounting.)</p>
<pre><code class="python">montecarlo = MonteCarlo(gamma=0.99)
</code></pre>

<p>Ok, let's see how <em>reward discounting</em> works.</p>
<p>Here we assume MonteCarlo method gets the episode of some task like below</p>
<pre><code>step0. agent at initial state &quot;s0&quot;
step1. agent take action &quot;a0&quot; at &quot;s0&quot; and move to next state &quot;s1&quot; and received reward 0.
step2. agent take action &quot;a1&quot; at &quot;s1&quot; and move to next state &quot;s2&quot; and received reward 0.
step3. agent take action &quot;a2&quot; at &quot;s2&quot; and move to terminal state &quot;s3&quot; and received reward 1.
</code></pre>

<p>The value of G at s0 (sum of rewards gained after s0) <strong>without reward discounting</strong> is</p>
<pre><code class="python">G = reward_at_step1 + reward_at_step2 + reward_at_step3
  = 0 + 0 + 1
  = 1
</code></pre>

<p>The value of G at s0 <strong>with reward discounting</strong> (if <code>gamma=0.99</code>) is</p>
<pre><code class="python">G = reward_at_step1 + gamma * reward_at_step2 + gamma**2 * reward_at_step3
  = 0 + 0.1 * 0 + 0.1**2 * 1
  = 0.9801
</code></pre>

<h2 id="value-function">Value function</h2>
<p>MonteCarlo method provides <strong>tabular</strong> and <strong>approximation</strong> type of value functions.  </p>
<h3 id="montecarlotabularactionvaluefunction">MonteCarloTabularActionValueFunction</h3>
<p>If your task is <em>tabular size</em>, you can use <code>MonteCarloTabularActionValueFunction</code>.</p>
<blockquote>
<p>If you can store the value of all state-action pair on the memory(array), your task is <strong>tabular</strong> size.</p>
</blockquote>
<p><code>MonteCarloTabularActionValueFunction</code> has 3 abstracted method to define the table size of your task.</p>
<ul>
<li><code>generate_initial_table</code> : initialize table object and return it here</li>
<li><code>fetch_value_from_table</code> : define how to fetch value from your table</li>
<li><code>insert_value_into_table</code> : define how to insert new value into your table</li>
</ul>
<p>If the shape of your state-action space is SxA, implementation would be like this.</p>
<pre><code class="python">class MyTabularActionValueFunction(MonteCarloTabularActionValueFunction):

    def generate_initial_table(self):
        return [[0 for j in range(A)] for i in range(S)]

    def fetch_value_from_table(self, table, state, action):
        return table[state][action]

    def insert_value_into_table(self, table, state, action, new_value):
        table[state][action] = new_value
</code></pre>

<h3 id="montecarloapproxactionvaluefunction">MonteCarloApproxActionValueFunction</h3>
<p>If your task is not <em>tabular</em> size, you use <code>MonteCarloApproxActionValueFunction</code>.</p>
<p><code>MonteCarloApproxActionValueFunction</code> has 3 abstracted methods. You would wrap some prediction model (ex. neuralnet) in these methods.</p>
<ul>
<li><code>construct_features</code> : transform state-action pair into feature representation</li>
<li><code>approx_predict_value</code> : predict value of state-action pair with prediction model you want to use</li>
<li><code>approx_backup</code> : update your model in supervised learning way with passed input and output pair</li>
</ul>
<p>The implementation with some neuralnet library would be like this.</p>
<pre><code class="python">class MyApproxActionValueFunction(MonteCarloApproxActionValueFunction):

    def setup(self):
        super(MazeApproxActionValueFunction, self).setup()
        self.neuralnet = build_neuralnet_in_some_way()

    def construct_features(self, state, action):
        feature1 = do_something(state, action)
        feature2 = do_anotherthing(state, action)
        return [feature1, feature2]

    def approx_predict_value(self, features):
        return self.neuralnet.predict(features)

    def approx_backup(self, features, backup_target, alpha):
        self.neuralnet.incremental_training(X=features, Y=backup_target)
</code></pre>

<h4 id="sample-code-to-start-learning">Sample code to start learning</h4>
<pre><code class="python">test_length = 1000
task = MyTask()
policy = EpsilonGreedyPolicy(eps=0.1)
value_func = MyTabularActionValueFunction()
algorithm = MonteCarlo(gamma=0.99)
algorithm.setup(task, policy, value_func)
algorithm.run_gpi(test_length)
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../sarsa/" class="btn btn-neutral float-right" title="Sarsa">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../about_algorithms/" class="btn btn-neutral" title="About Algorithms"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/ishikota/kyoka" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../about_algorithms/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../sarsa/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../js/theme.js"></script>

</body>
</html>
