#!/usr/local/bin/python

# Resolve path configucation
import os
import sys
import argparse

root = os.path.join(os.path.dirname(__file__), "../"*3)
src_path = os.path.join(root, "kyoka")
sample_path = os.path.join(root, "sample")
sys.path.append(root)
sys.path.append(src_path)
sys.path.append(sample_path)

import logging as log
log.basicConfig(format='[%(levelname)s] %(message)s', level=log.INFO)

from kyoka.algorithm.montecarlo.montecarlo import MonteCarlo
from kyoka.algorithm.td_learning.sarsa import Sarsa
from kyoka.algorithm.td_learning.q_learning import QLearning
from kyoka.algorithm.td_learning.sarsa_lambda import SarsaLambda
from kyoka.algorithm.td_learning.q_lambda import QLambda

from kyoka.policy.greedy_policy import GreedyPolicy
from kyoka.policy.epsilon_greedy_policy import EpsilonGreedyPolicy
from kyoka.finish_rule.watch_iteration_count import WatchIterationCount

from sample.ticktacktoe.ticktacktoe_domain import TickTackToeDomain
from sample.ticktacktoe.ticktacktoe_table_value_function import TickTackToeTableValueFunction
from sample.ticktacktoe.ticktacktoe_helper import TickTackToeHelper
from sample.ticktacktoe.ticktacktoe_manual_policy import TickTackToeManualPolicy
from sample.ticktacktoe.ticktacktoe_perfect_policy import TickTackToePerfectPolicy
from sample.ticktacktoe.ticktacktoe_performance_logger import TickTackToePerformanceLogger

SUPPORT_ALGORITHM = ["montecarlo", "sarsa", "qlearning", "sarsalambda", "qlambda", "minimax"]

parser = argparse.ArgumentParser(description="Setup game configutation")
parser.add_argument("--algo", required=True, help=" or ".join(['"%s"' % algo for algo in SUPPORT_ALGORITHM]))
parser.add_argument("--firstplayer", required=False, default="y", help='pass "yes" if you want to train first player (default is "yes")')
args = parser.parse_args()
algo = args.algo
is_first_player = True if args.firstplayer.lower() in ["yes", "y"] else False
if algo not in SUPPORT_ALGORITHM:
  raise ValueError("unknown algorithm [%s] passed." % algo)

TEST_LENGTH = 100
TEST_GAME_COUNT = 10
domain = TickTackToeDomain(is_first_player=is_first_player)
value_func = TickTackToeTableValueFunction()
value_func.setUp()
policy = EpsilonGreedyPolicy(domain, value_func, eps=0.7)
watch_iteration = WatchIterationCount(target_count=TEST_LENGTH, log_interval=10)
finish_rules = [watch_iteration]

RL_algo = {
    "montecarlo": lambda : MonteCarlo(),
    "sarsa": lambda : Sarsa(),
    "qlearning": lambda :QLearning(),
    "sarsalambda": lambda :SarsaLambda(),
    "qlambda": lambda :QLambda()
}[algo]()

callback = TickTackToePerformanceLogger()
callback.set_is_first_player(is_first_player)
callback.set_test_game_count(TEST_GAME_COUNT)
RL_algo.set_gpi_callback(callback)

log.info("start to measure performnce for %d episode" % TEST_LENGTH)
RL_algo.GPI(domain, policy, value_func, finish_rules)
log.info("finished to measure performnce for %d episode" % TEST_LENGTH)
log.info("performance log = %s" % callback.game_log)

