#!/usr/local/bin/python

# Resolve path configucation
import os
import sys
import argparse

root = os.path.join(os.path.dirname(__file__), "../"*3)
src_path = os.path.join(root, "kyoka")
sample_path = os.path.join(root, "sample")
sys.path.append(root)
sys.path.append(src_path)
sys.path.append(sample_path)

import logging as log
log.basicConfig(format='[%(levelname)s] %(message)s', level=log.DEBUG)

from kyoka.algorithm.montecarlo.montecarlo import MonteCarlo
from kyoka.algorithm.td_learning.sarsa import Sarsa
from kyoka.algorithm.td_learning.q_learning import QLearning
from kyoka.algorithm.td_learning.sarsa_lambda import SarsaLambda
from kyoka.algorithm.td_learning.q_lambda import QLambda

from kyoka.policy.epsilon_greedy_policy import EpsilonGreedyPolicy
from kyoka.finish_rule.watch_iteration_count import WatchIterationCount

from sample.maze.maze_domain import MazeDomain
from sample.maze.maze_table_value_function import MazeTableValueFunction
from sample.maze.maze_helper import MazeHelper
from sample.maze.maze_performance_logger import MazePerformanceLogger
from sample.maze.maze_transformer import MazeTransformer

SUPPORT_ALGORITHM = ["human", "montecarlo", "sarsa", "qlearning", "sarsalambda", "qlambda"]
SUPPORT_MAZE_TYPE = ["dyna", "blocking", "shortcut"]

parser = argparse.ArgumentParser(description="Specify RL algorithm to use")
parser.add_argument("--algo", required=True, help=" or ".join(['"%s"' % algo for algo in SUPPORT_ALGORITHM]))
parser.add_argument("--maze", required=True, help='pass maze file path')
args = parser.parse_args()
algo = args.algo
maze_type = args.maze
if algo not in SUPPORT_ALGORITHM:
  raise ValueError("unknown algorithm [%s] passed." % algo)
if maze_type not in SUPPORT_MAZE_TYPE:
  raise ValueError("unknown maze type [%s] passed." % maze_type)

VALUE_FUNC_FILE_PATH = "%s_%s_maze_value_function_data.pickle" % (algo, maze_type)
VALUE_FUNC_SAVE_PATH = os.path.join(os.path.dirname(__file__), VALUE_FUNC_FILE_PATH)
INTERRUPTION_MONITOR_FILE_PATH = os.path.join(os.path.dirname(__file__), "interruption_order.txt")
MAZE_FILE_PATH = os.path.join(os.path.dirname(__file__), maze_type + ".txt")

domain = MazeDomain()
domain.read_maze(MAZE_FILE_PATH)
value_func = MazeTableValueFunction(domain.get_maze_shape())
value_func.setUp()

TEST_LENGTH = 100
policy = EpsilonGreedyPolicy(eps=0.1)
callbacks = [MazePerformanceLogger()]
if maze_type in ["blocking", "shortcut"]:
  transfomer = MazeTransformer()
  transformed_maze_filepath = MAZE_FILE_PATH[:-len(".txt")] + "_transformed.txt"
  transfomer.set_transformation(50, transformed_maze_filepath)
  callbacks.append(transfomer)

RL_algo = {
    "montecarlo": lambda : MonteCarlo(),
    "sarsa": lambda : Sarsa(),
    "qlearning": lambda :QLearning(),
    "sarsalambda": lambda :SarsaLambda(),
    "qlambda": lambda :QLambda()
}[algo]()

log.info("start to measure performnce for %d episode" % TEST_LENGTH)
RL_algo.setUp(domain, policy, value_func)
RL_algo.run_gpi(TEST_LENGTH, callbacks=callbacks)
log.info("finished to measure performnce for %d episode" % TEST_LENGTH)
log.info("performance_log = %s" % callbacks[0].step_log)
log.info("Policy which agent learned is like this.\n%s" % MazeHelper.visualize_policy(domain, value_func))

